{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da48eadb",
   "metadata": {},
   "source": [
    "### 1 - Importa bibliotecas necessárias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8de20ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e7bc70",
   "metadata": {},
   "source": [
    "### 2 - Carrega dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "605cf6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nome do arquivo\n",
    "\n",
    "dir_treino = 'Modulo_Fase_TREINO.txt' \n",
    "dir_teste ='Modulo_Fase_TESTE.txt'\n",
    "\n",
    "\n",
    "# Informa o cabeçalho das colunas\n",
    "colunas = ['ID_TRILHA', 'G_SAÚDE', 'G_ISOLAMENTO', 'G_TRILHA', 'N_ISOLAMENTO', 'N_TRILHA', 'M_225886', 'M_246180', 'M_268298', 'M_292402', 'M_318672', 'M_347302', 'M_378504', 'M_412509', 'M_449569', 'M_489959', 'M_533978', 'M_581951', 'M_634235', 'M_691215', 'M_753315', 'M_820994', 'M_894753', 'M_975139', 'M_1062747', 'M_1158226', 'M_1262283', 'M_1375688', 'M_1499282', 'M_1633980', 'M_1780779', 'M_1940767', 'M_2115128', 'M_2305154', 'M_2512253', 'M_2737957', 'M_2983939', 'M_3252021', 'M_3544187', 'M_3862602', 'M_4209624', 'M_4587823', 'F_225886', 'F_246180', 'F_268298', 'F_292402', 'F_318672', 'F_347302', 'F_378504', 'F_412509', 'F_449569', 'F_489959', 'F_533978', 'F_581951', 'F_634235', 'F_691215', 'F_753315', 'F_820994', 'F_894753', 'F_975139', 'F_1062747', 'F_1158226', 'F_1262283', 'F_1375688', 'F_1499282', 'F_1633980', 'F_1780779', 'F_1940767', 'F_2115128', 'F_2305154', 'F_2512253', 'F_2737957', 'F_2983939', 'F_3252021', 'F_3544187', 'F_3862602', 'F_4209624', 'F_4587823']\n",
    "# Carrega uma base de dados - TREINO\n",
    "dataset_treino = pd.read_csv(dir_treino, names=colunas, skiprows=0, delimiter=';') \n",
    "\n",
    "#Carrega uma base de dados - TESTE\n",
    "dataset_teste = pd.read_csv(dir_teste, names=colunas, skiprows=0, delimiter=';')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "65d8f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mistura a ordem das  linhas \n",
    "\n",
    "# Definir a semente\n",
    "seed = 7\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "# Misturar as linhas do DataFrame\n",
    "dataset_treino = dataset_treino.sample(frac=1).reset_index(drop=True)\n",
    "dataset_teste = dataset_teste.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d47967f",
   "metadata": {},
   "source": [
    "#### 2.1 - Visualização dos dados e analise resumida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f5f32283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_TRILHA</th>\n",
       "      <th>G_SAÚDE</th>\n",
       "      <th>G_ISOLAMENTO</th>\n",
       "      <th>G_TRILHA</th>\n",
       "      <th>N_ISOLAMENTO</th>\n",
       "      <th>N_TRILHA</th>\n",
       "      <th>M_225886</th>\n",
       "      <th>M_246180</th>\n",
       "      <th>M_268298</th>\n",
       "      <th>M_292402</th>\n",
       "      <th>...</th>\n",
       "      <th>F_2115128</th>\n",
       "      <th>F_2305154</th>\n",
       "      <th>F_2512253</th>\n",
       "      <th>F_2737957</th>\n",
       "      <th>F_2983939</th>\n",
       "      <th>F_3252021</th>\n",
       "      <th>F_3544187</th>\n",
       "      <th>F_3862602</th>\n",
       "      <th>F_4209624</th>\n",
       "      <th>F_4587823</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I2_C3_T7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.75</td>\n",
       "      <td>156335.0</td>\n",
       "      <td>143802.0</td>\n",
       "      <td>132243.0</td>\n",
       "      <td>121611.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.11</td>\n",
       "      <td>-76.14</td>\n",
       "      <td>-75.10</td>\n",
       "      <td>-73.95</td>\n",
       "      <td>-72.59</td>\n",
       "      <td>-71.16</td>\n",
       "      <td>-69.51</td>\n",
       "      <td>-67.70</td>\n",
       "      <td>-65.79</td>\n",
       "      <td>-64.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I1_C0_T1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>160660.0</td>\n",
       "      <td>147742.0</td>\n",
       "      <td>135860.0</td>\n",
       "      <td>124897.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-75.96</td>\n",
       "      <td>-74.88</td>\n",
       "      <td>-73.72</td>\n",
       "      <td>-72.41</td>\n",
       "      <td>-70.87</td>\n",
       "      <td>-69.16</td>\n",
       "      <td>-67.32</td>\n",
       "      <td>-65.28</td>\n",
       "      <td>-63.04</td>\n",
       "      <td>-60.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I4_C0_T5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>197720.0</td>\n",
       "      <td>181686.0</td>\n",
       "      <td>167597.0</td>\n",
       "      <td>154044.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-76.01</td>\n",
       "      <td>-74.98</td>\n",
       "      <td>-73.86</td>\n",
       "      <td>-72.61</td>\n",
       "      <td>-71.08</td>\n",
       "      <td>-69.42</td>\n",
       "      <td>-67.60</td>\n",
       "      <td>-65.62</td>\n",
       "      <td>-63.43</td>\n",
       "      <td>-60.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I3_C3_T3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>193522.0</td>\n",
       "      <td>177844.0</td>\n",
       "      <td>163527.0</td>\n",
       "      <td>150331.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.31</td>\n",
       "      <td>-76.35</td>\n",
       "      <td>-75.33</td>\n",
       "      <td>-74.18</td>\n",
       "      <td>-72.82</td>\n",
       "      <td>-71.38</td>\n",
       "      <td>-69.72</td>\n",
       "      <td>-67.92</td>\n",
       "      <td>-66.05</td>\n",
       "      <td>-64.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I2_C3_T11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.75</td>\n",
       "      <td>134048.0</td>\n",
       "      <td>123242.0</td>\n",
       "      <td>113351.0</td>\n",
       "      <td>104234.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.22</td>\n",
       "      <td>-76.25</td>\n",
       "      <td>-75.20</td>\n",
       "      <td>-74.04</td>\n",
       "      <td>-72.70</td>\n",
       "      <td>-71.25</td>\n",
       "      <td>-69.61</td>\n",
       "      <td>-67.81</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>-64.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_TRILHA  G_SAÚDE  G_ISOLAMENTO  G_TRILHA  N_ISOLAMENTO  N_TRILHA  \\\n",
       "0   I2_C3_T7      0.0           2.0       7.0           1.0      1.75   \n",
       "1   I1_C0_T1      1.0           1.0       1.0           0.5      0.25   \n",
       "2   I4_C0_T5      1.0           4.0       5.0           2.0      1.25   \n",
       "3   I3_C3_T3      0.0           3.0       3.0           1.5      0.75   \n",
       "4  I2_C3_T11      0.0           2.0      11.0           1.0      2.75   \n",
       "\n",
       "   M_225886  M_246180  M_268298  M_292402  ...  F_2115128  F_2305154  \\\n",
       "0  156335.0  143802.0  132243.0  121611.0  ...     -77.11     -76.14   \n",
       "1  160660.0  147742.0  135860.0  124897.0  ...     -75.96     -74.88   \n",
       "2  197720.0  181686.0  167597.0  154044.0  ...     -76.01     -74.98   \n",
       "3  193522.0  177844.0  163527.0  150331.0  ...     -77.31     -76.35   \n",
       "4  134048.0  123242.0  113351.0  104234.0  ...     -77.22     -76.25   \n",
       "\n",
       "   F_2512253  F_2737957  F_2983939  F_3252021  F_3544187  F_3862602  \\\n",
       "0     -75.10     -73.95     -72.59     -71.16     -69.51     -67.70   \n",
       "1     -73.72     -72.41     -70.87     -69.16     -67.32     -65.28   \n",
       "2     -73.86     -72.61     -71.08     -69.42     -67.60     -65.62   \n",
       "3     -75.33     -74.18     -72.82     -71.38     -69.72     -67.92   \n",
       "4     -75.20     -74.04     -72.70     -71.25     -69.61     -67.81   \n",
       "\n",
       "   F_4209624  F_4587823  \n",
       "0     -65.79     -64.02  \n",
       "1     -63.04     -60.51  \n",
       "2     -63.43     -60.98  \n",
       "3     -66.05     -64.39  \n",
       "4     -65.91     -64.05  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# primeiras linhas do dataset - TREINO\n",
    "dataset_treino.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8b29444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_TRILHA</th>\n",
       "      <th>G_SAÚDE</th>\n",
       "      <th>G_ISOLAMENTO</th>\n",
       "      <th>G_TRILHA</th>\n",
       "      <th>N_ISOLAMENTO</th>\n",
       "      <th>N_TRILHA</th>\n",
       "      <th>M_225886</th>\n",
       "      <th>M_246180</th>\n",
       "      <th>M_268298</th>\n",
       "      <th>M_292402</th>\n",
       "      <th>...</th>\n",
       "      <th>F_2115128</th>\n",
       "      <th>F_2305154</th>\n",
       "      <th>F_2512253</th>\n",
       "      <th>F_2737957</th>\n",
       "      <th>F_2983939</th>\n",
       "      <th>F_3252021</th>\n",
       "      <th>F_3544187</th>\n",
       "      <th>F_3862602</th>\n",
       "      <th>F_4209624</th>\n",
       "      <th>F_4587823</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I2_C2_T11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.75</td>\n",
       "      <td>148575.0</td>\n",
       "      <td>136711.0</td>\n",
       "      <td>125671.0</td>\n",
       "      <td>115588.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.14</td>\n",
       "      <td>-76.18</td>\n",
       "      <td>-75.14</td>\n",
       "      <td>-73.97</td>\n",
       "      <td>-72.62</td>\n",
       "      <td>-71.18</td>\n",
       "      <td>-69.53</td>\n",
       "      <td>-67.71</td>\n",
       "      <td>-65.80</td>\n",
       "      <td>-64.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I3_C1_T9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.25</td>\n",
       "      <td>167194.0</td>\n",
       "      <td>153726.0</td>\n",
       "      <td>141303.0</td>\n",
       "      <td>129867.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.51</td>\n",
       "      <td>-76.55</td>\n",
       "      <td>-75.52</td>\n",
       "      <td>-74.37</td>\n",
       "      <td>-73.01</td>\n",
       "      <td>-71.57</td>\n",
       "      <td>-69.93</td>\n",
       "      <td>-68.12</td>\n",
       "      <td>-66.24</td>\n",
       "      <td>-64.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I2_C0_T8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>143487.0</td>\n",
       "      <td>132062.0</td>\n",
       "      <td>121487.0</td>\n",
       "      <td>111750.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-75.79</td>\n",
       "      <td>-74.73</td>\n",
       "      <td>-73.58</td>\n",
       "      <td>-72.31</td>\n",
       "      <td>-70.81</td>\n",
       "      <td>-69.14</td>\n",
       "      <td>-67.32</td>\n",
       "      <td>-65.32</td>\n",
       "      <td>-63.12</td>\n",
       "      <td>-60.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I3_C4_T3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>174040.0</td>\n",
       "      <td>160035.0</td>\n",
       "      <td>147157.0</td>\n",
       "      <td>135301.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.31</td>\n",
       "      <td>-76.36</td>\n",
       "      <td>-75.35</td>\n",
       "      <td>-74.21</td>\n",
       "      <td>-72.82</td>\n",
       "      <td>-71.39</td>\n",
       "      <td>-69.73</td>\n",
       "      <td>-67.90</td>\n",
       "      <td>-66.04</td>\n",
       "      <td>-64.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I1_C2_T11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.75</td>\n",
       "      <td>141582.0</td>\n",
       "      <td>130170.0</td>\n",
       "      <td>119727.0</td>\n",
       "      <td>110060.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-75.28</td>\n",
       "      <td>-74.18</td>\n",
       "      <td>-73.01</td>\n",
       "      <td>-71.69</td>\n",
       "      <td>-70.10</td>\n",
       "      <td>-68.43</td>\n",
       "      <td>-66.49</td>\n",
       "      <td>-64.38</td>\n",
       "      <td>-62.24</td>\n",
       "      <td>-60.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_TRILHA  G_SAÚDE  G_ISOLAMENTO  G_TRILHA  N_ISOLAMENTO  N_TRILHA  \\\n",
       "0  I2_C2_T11      0.0           2.0      11.0           1.0      2.75   \n",
       "1   I3_C1_T9      0.0           3.0       9.0           1.5      2.25   \n",
       "2   I2_C0_T8      1.0           2.0       8.0           1.0      2.00   \n",
       "3   I3_C4_T3      0.0           3.0       3.0           1.5      0.75   \n",
       "4  I1_C2_T11      0.0           1.0      11.0           0.5      2.75   \n",
       "\n",
       "   M_225886  M_246180  M_268298  M_292402  ...  F_2115128  F_2305154  \\\n",
       "0  148575.0  136711.0  125671.0  115588.0  ...     -77.14     -76.18   \n",
       "1  167194.0  153726.0  141303.0  129867.0  ...     -77.51     -76.55   \n",
       "2  143487.0  132062.0  121487.0  111750.0  ...     -75.79     -74.73   \n",
       "3  174040.0  160035.0  147157.0  135301.0  ...     -77.31     -76.36   \n",
       "4  141582.0  130170.0  119727.0  110060.0  ...     -75.28     -74.18   \n",
       "\n",
       "   F_2512253  F_2737957  F_2983939  F_3252021  F_3544187  F_3862602  \\\n",
       "0     -75.14     -73.97     -72.62     -71.18     -69.53     -67.71   \n",
       "1     -75.52     -74.37     -73.01     -71.57     -69.93     -68.12   \n",
       "2     -73.58     -72.31     -70.81     -69.14     -67.32     -65.32   \n",
       "3     -75.35     -74.21     -72.82     -71.39     -69.73     -67.90   \n",
       "4     -73.01     -71.69     -70.10     -68.43     -66.49     -64.38   \n",
       "\n",
       "   F_4209624  F_4587823  \n",
       "0     -65.80     -64.01  \n",
       "1     -66.24     -64.52  \n",
       "2     -63.12     -60.65  \n",
       "3     -66.04     -64.38  \n",
       "4     -62.24     -60.42  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# primeiras linhas do dataset - TREINO\n",
    "dataset_teste.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "30386fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID_TRILHA        object\n",
       "G_SAÚDE         float64\n",
       "G_ISOLAMENTO    float64\n",
       "G_TRILHA        float64\n",
       "N_ISOLAMENTO    float64\n",
       "                 ...   \n",
       "F_3252021       float64\n",
       "F_3544187       float64\n",
       "F_3862602       float64\n",
       "F_4209624       float64\n",
       "F_4587823       float64\n",
       "Length: 78, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tipos de cada atributo\n",
    "dataset_treino.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6612466e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID_TRILHA        object\n",
       "G_SAÚDE         float64\n",
       "G_ISOLAMENTO    float64\n",
       "G_TRILHA        float64\n",
       "N_ISOLAMENTO    float64\n",
       "                 ...   \n",
       "F_3252021       float64\n",
       "F_3544187       float64\n",
       "F_3862602       float64\n",
       "F_4209624       float64\n",
       "F_4587823       float64\n",
       "Length: 78, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tipos de cada atributo\n",
    "dataset_teste.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "76f4de5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G_SAÚDE</th>\n",
       "      <th>G_ISOLAMENTO</th>\n",
       "      <th>G_TRILHA</th>\n",
       "      <th>N_ISOLAMENTO</th>\n",
       "      <th>N_TRILHA</th>\n",
       "      <th>M_225886</th>\n",
       "      <th>M_246180</th>\n",
       "      <th>M_268298</th>\n",
       "      <th>M_292402</th>\n",
       "      <th>M_318672</th>\n",
       "      <th>...</th>\n",
       "      <th>F_2115128</th>\n",
       "      <th>F_2305154</th>\n",
       "      <th>F_2512253</th>\n",
       "      <th>F_2737957</th>\n",
       "      <th>F_2983939</th>\n",
       "      <th>F_3252021</th>\n",
       "      <th>F_3544187</th>\n",
       "      <th>F_3862602</th>\n",
       "      <th>F_4209624</th>\n",
       "      <th>F_4587823</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.00000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.00000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.392857</td>\n",
       "      <td>6.095238</td>\n",
       "      <td>1.196429</td>\n",
       "      <td>1.523810</td>\n",
       "      <td>164460.083333</td>\n",
       "      <td>151259.904762</td>\n",
       "      <td>139110.845238</td>\n",
       "      <td>127934.77381</td>\n",
       "      <td>117659.035714</td>\n",
       "      <td>...</td>\n",
       "      <td>-76.489405</td>\n",
       "      <td>-75.479286</td>\n",
       "      <td>-74.394524</td>\n",
       "      <td>-73.182500</td>\n",
       "      <td>-71.731071</td>\n",
       "      <td>-70.168690</td>\n",
       "      <td>-68.406548</td>\n",
       "      <td>-66.49381</td>\n",
       "      <td>-64.446786</td>\n",
       "      <td>-62.367976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.503003</td>\n",
       "      <td>1.075666</td>\n",
       "      <td>3.459295</td>\n",
       "      <td>0.537833</td>\n",
       "      <td>0.864824</td>\n",
       "      <td>25843.328626</td>\n",
       "      <td>23784.239370</td>\n",
       "      <td>21882.445875</td>\n",
       "      <td>20125.28636</td>\n",
       "      <td>18510.962860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595866</td>\n",
       "      <td>0.643958</td>\n",
       "      <td>0.699364</td>\n",
       "      <td>0.765583</td>\n",
       "      <td>0.833188</td>\n",
       "      <td>0.944289</td>\n",
       "      <td>1.010560</td>\n",
       "      <td>1.10410</td>\n",
       "      <td>1.268430</td>\n",
       "      <td>1.645666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>123545.000000</td>\n",
       "      <td>113604.000000</td>\n",
       "      <td>104458.000000</td>\n",
       "      <td>96073.00000</td>\n",
       "      <td>88320.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.560000</td>\n",
       "      <td>-76.600000</td>\n",
       "      <td>-75.580000</td>\n",
       "      <td>-74.430000</td>\n",
       "      <td>-73.070000</td>\n",
       "      <td>-71.640000</td>\n",
       "      <td>-69.980000</td>\n",
       "      <td>-68.17000</td>\n",
       "      <td>-66.320000</td>\n",
       "      <td>-64.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>145808.500000</td>\n",
       "      <td>134106.250000</td>\n",
       "      <td>123372.000000</td>\n",
       "      <td>113433.25000</td>\n",
       "      <td>104318.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.112500</td>\n",
       "      <td>-76.145000</td>\n",
       "      <td>-75.110000</td>\n",
       "      <td>-73.952500</td>\n",
       "      <td>-72.592500</td>\n",
       "      <td>-71.160000</td>\n",
       "      <td>-69.502500</td>\n",
       "      <td>-67.69000</td>\n",
       "      <td>-65.800000</td>\n",
       "      <td>-64.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>159963.500000</td>\n",
       "      <td>147191.500000</td>\n",
       "      <td>135394.500000</td>\n",
       "      <td>124499.50000</td>\n",
       "      <td>114487.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>-76.225000</td>\n",
       "      <td>-75.185000</td>\n",
       "      <td>-74.080000</td>\n",
       "      <td>-72.825000</td>\n",
       "      <td>-71.305000</td>\n",
       "      <td>-69.620000</td>\n",
       "      <td>-67.815000</td>\n",
       "      <td>-65.82500</td>\n",
       "      <td>-63.640000</td>\n",
       "      <td>-61.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>176586.000000</td>\n",
       "      <td>162459.250000</td>\n",
       "      <td>149400.250000</td>\n",
       "      <td>137374.25000</td>\n",
       "      <td>126315.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-75.960000</td>\n",
       "      <td>-74.887500</td>\n",
       "      <td>-73.745000</td>\n",
       "      <td>-72.472500</td>\n",
       "      <td>-70.950000</td>\n",
       "      <td>-69.275000</td>\n",
       "      <td>-67.457500</td>\n",
       "      <td>-65.46750</td>\n",
       "      <td>-63.260000</td>\n",
       "      <td>-60.797500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>246198.000000</td>\n",
       "      <td>226586.000000</td>\n",
       "      <td>208371.000000</td>\n",
       "      <td>191581.00000</td>\n",
       "      <td>176185.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-75.740000</td>\n",
       "      <td>-74.680000</td>\n",
       "      <td>-73.540000</td>\n",
       "      <td>-72.270000</td>\n",
       "      <td>-70.760000</td>\n",
       "      <td>-69.100000</td>\n",
       "      <td>-67.280000</td>\n",
       "      <td>-65.24000</td>\n",
       "      <td>-63.000000</td>\n",
       "      <td>-60.490000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         G_SAÚDE  G_ISOLAMENTO   G_TRILHA  N_ISOLAMENTO   N_TRILHA  \\\n",
       "count  84.000000     84.000000  84.000000     84.000000  84.000000   \n",
       "mean    0.500000      2.392857   6.095238      1.196429   1.523810   \n",
       "std     0.503003      1.075666   3.459295      0.537833   0.864824   \n",
       "min     0.000000      1.000000   1.000000      0.500000   0.250000   \n",
       "25%     0.000000      1.000000   3.000000      0.500000   0.750000   \n",
       "50%     0.500000      2.000000   6.000000      1.000000   1.500000   \n",
       "75%     1.000000      3.000000   9.000000      1.500000   2.250000   \n",
       "max     1.000000      4.000000  12.000000      2.000000   3.000000   \n",
       "\n",
       "            M_225886       M_246180       M_268298      M_292402  \\\n",
       "count      84.000000      84.000000      84.000000      84.00000   \n",
       "mean   164460.083333  151259.904762  139110.845238  127934.77381   \n",
       "std     25843.328626   23784.239370   21882.445875   20125.28636   \n",
       "min    123545.000000  113604.000000  104458.000000   96073.00000   \n",
       "25%    145808.500000  134106.250000  123372.000000  113433.25000   \n",
       "50%    159963.500000  147191.500000  135394.500000  124499.50000   \n",
       "75%    176586.000000  162459.250000  149400.250000  137374.25000   \n",
       "max    246198.000000  226586.000000  208371.000000  191581.00000   \n",
       "\n",
       "            M_318672  ...  F_2115128  F_2305154  F_2512253  F_2737957  \\\n",
       "count      84.000000  ...  84.000000  84.000000  84.000000  84.000000   \n",
       "mean   117659.035714  ... -76.489405 -75.479286 -74.394524 -73.182500   \n",
       "std     18510.962860  ...   0.595866   0.643958   0.699364   0.765583   \n",
       "min     88320.000000  ... -77.560000 -76.600000 -75.580000 -74.430000   \n",
       "25%    104318.750000  ... -77.112500 -76.145000 -75.110000 -73.952500   \n",
       "50%    114487.500000  ... -76.225000 -75.185000 -74.080000 -72.825000   \n",
       "75%    126315.000000  ... -75.960000 -74.887500 -73.745000 -72.472500   \n",
       "max    176185.000000  ... -75.740000 -74.680000 -73.540000 -72.270000   \n",
       "\n",
       "       F_2983939  F_3252021  F_3544187  F_3862602  F_4209624  F_4587823  \n",
       "count  84.000000  84.000000  84.000000   84.00000  84.000000  84.000000  \n",
       "mean  -71.731071 -70.168690 -68.406548  -66.49381 -64.446786 -62.367976  \n",
       "std     0.833188   0.944289   1.010560    1.10410   1.268430   1.645666  \n",
       "min   -73.070000 -71.640000 -69.980000  -68.17000 -66.320000 -64.750000  \n",
       "25%   -72.592500 -71.160000 -69.502500  -67.69000 -65.800000 -64.050000  \n",
       "50%   -71.305000 -69.620000 -67.815000  -65.82500 -63.640000 -61.580000  \n",
       "75%   -70.950000 -69.275000 -67.457500  -65.46750 -63.260000 -60.797500  \n",
       "max   -70.760000 -69.100000 -67.280000  -65.24000 -63.000000 -60.490000  \n",
       "\n",
       "[8 rows x 77 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dados básicos - - TREINO\n",
    "dataset_treino.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4acead96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    42\n",
       "1.0    42\n",
       "Name: G_SAÚDE, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_treino['G_SAÚDE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3265a190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G_SAÚDE</th>\n",
       "      <th>G_ISOLAMENTO</th>\n",
       "      <th>G_TRILHA</th>\n",
       "      <th>N_ISOLAMENTO</th>\n",
       "      <th>N_TRILHA</th>\n",
       "      <th>M_225886</th>\n",
       "      <th>M_246180</th>\n",
       "      <th>M_268298</th>\n",
       "      <th>M_292402</th>\n",
       "      <th>M_318672</th>\n",
       "      <th>...</th>\n",
       "      <th>F_2115128</th>\n",
       "      <th>F_2305154</th>\n",
       "      <th>F_2512253</th>\n",
       "      <th>F_2737957</th>\n",
       "      <th>F_2983939</th>\n",
       "      <th>F_3252021</th>\n",
       "      <th>F_3544187</th>\n",
       "      <th>F_3862602</th>\n",
       "      <th>F_4209624</th>\n",
       "      <th>F_4587823</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.00000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.171429</td>\n",
       "      <td>2.285714</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>163224.60000</td>\n",
       "      <td>150097.142857</td>\n",
       "      <td>138056.085714</td>\n",
       "      <td>126946.942857</td>\n",
       "      <td>116727.257143</td>\n",
       "      <td>...</td>\n",
       "      <td>-76.877714</td>\n",
       "      <td>-75.893714</td>\n",
       "      <td>-74.838286</td>\n",
       "      <td>-73.658571</td>\n",
       "      <td>-72.259714</td>\n",
       "      <td>-70.768571</td>\n",
       "      <td>-69.070000</td>\n",
       "      <td>-67.214857</td>\n",
       "      <td>-65.264571</td>\n",
       "      <td>-63.398286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.382385</td>\n",
       "      <td>0.859939</td>\n",
       "      <td>3.397231</td>\n",
       "      <td>0.429970</td>\n",
       "      <td>0.849308</td>\n",
       "      <td>22286.58588</td>\n",
       "      <td>20479.124802</td>\n",
       "      <td>18842.854355</td>\n",
       "      <td>17309.958156</td>\n",
       "      <td>15899.351476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.582485</td>\n",
       "      <td>0.625493</td>\n",
       "      <td>0.672819</td>\n",
       "      <td>0.726418</td>\n",
       "      <td>0.788817</td>\n",
       "      <td>0.877416</td>\n",
       "      <td>0.952341</td>\n",
       "      <td>1.034236</td>\n",
       "      <td>1.160645</td>\n",
       "      <td>1.410057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>124299.00000</td>\n",
       "      <td>114338.000000</td>\n",
       "      <td>105136.000000</td>\n",
       "      <td>96720.000000</td>\n",
       "      <td>88926.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.510000</td>\n",
       "      <td>-76.550000</td>\n",
       "      <td>-75.520000</td>\n",
       "      <td>-74.370000</td>\n",
       "      <td>-73.010000</td>\n",
       "      <td>-71.570000</td>\n",
       "      <td>-69.930000</td>\n",
       "      <td>-68.120000</td>\n",
       "      <td>-66.280000</td>\n",
       "      <td>-64.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>148269.00000</td>\n",
       "      <td>136388.500000</td>\n",
       "      <td>125404.000000</td>\n",
       "      <td>115316.500000</td>\n",
       "      <td>106051.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.220000</td>\n",
       "      <td>-76.260000</td>\n",
       "      <td>-75.230000</td>\n",
       "      <td>-74.085000</td>\n",
       "      <td>-72.710000</td>\n",
       "      <td>-71.270000</td>\n",
       "      <td>-69.610000</td>\n",
       "      <td>-67.810000</td>\n",
       "      <td>-65.950000</td>\n",
       "      <td>-64.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>160626.00000</td>\n",
       "      <td>147502.000000</td>\n",
       "      <td>135850.000000</td>\n",
       "      <td>125048.000000</td>\n",
       "      <td>114883.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.140000</td>\n",
       "      <td>-76.180000</td>\n",
       "      <td>-75.140000</td>\n",
       "      <td>-73.970000</td>\n",
       "      <td>-72.620000</td>\n",
       "      <td>-71.180000</td>\n",
       "      <td>-69.520000</td>\n",
       "      <td>-67.690000</td>\n",
       "      <td>-65.800000</td>\n",
       "      <td>-64.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>179110.50000</td>\n",
       "      <td>164703.000000</td>\n",
       "      <td>151485.500000</td>\n",
       "      <td>139288.500000</td>\n",
       "      <td>128092.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-76.775000</td>\n",
       "      <td>-75.800000</td>\n",
       "      <td>-74.760000</td>\n",
       "      <td>-73.600000</td>\n",
       "      <td>-72.240000</td>\n",
       "      <td>-70.785000</td>\n",
       "      <td>-69.050000</td>\n",
       "      <td>-67.220000</td>\n",
       "      <td>-65.295000</td>\n",
       "      <td>-63.335000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>207257.00000</td>\n",
       "      <td>190447.000000</td>\n",
       "      <td>175266.000000</td>\n",
       "      <td>161116.000000</td>\n",
       "      <td>148136.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-75.280000</td>\n",
       "      <td>-74.180000</td>\n",
       "      <td>-73.010000</td>\n",
       "      <td>-71.690000</td>\n",
       "      <td>-70.100000</td>\n",
       "      <td>-68.430000</td>\n",
       "      <td>-66.490000</td>\n",
       "      <td>-64.380000</td>\n",
       "      <td>-62.240000</td>\n",
       "      <td>-60.420000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         G_SAÚDE  G_ISOLAMENTO   G_TRILHA  N_ISOLAMENTO   N_TRILHA  \\\n",
       "count  35.000000     35.000000  35.000000     35.000000  35.000000   \n",
       "mean    0.171429      2.285714   6.600000      1.142857   1.650000   \n",
       "std     0.382385      0.859939   3.397231      0.429970   0.849308   \n",
       "min     0.000000      1.000000   1.000000      0.500000   0.250000   \n",
       "25%     0.000000      2.000000   3.500000      1.000000   0.875000   \n",
       "50%     0.000000      2.000000   7.000000      1.000000   1.750000   \n",
       "75%     0.000000      3.000000   9.500000      1.500000   2.375000   \n",
       "max     1.000000      4.000000  11.000000      2.000000   2.750000   \n",
       "\n",
       "           M_225886       M_246180       M_268298       M_292402  \\\n",
       "count      35.00000      35.000000      35.000000      35.000000   \n",
       "mean   163224.60000  150097.142857  138056.085714  126946.942857   \n",
       "std     22286.58588   20479.124802   18842.854355   17309.958156   \n",
       "min    124299.00000  114338.000000  105136.000000   96720.000000   \n",
       "25%    148269.00000  136388.500000  125404.000000  115316.500000   \n",
       "50%    160626.00000  147502.000000  135850.000000  125048.000000   \n",
       "75%    179110.50000  164703.000000  151485.500000  139288.500000   \n",
       "max    207257.00000  190447.000000  175266.000000  161116.000000   \n",
       "\n",
       "            M_318672  ...  F_2115128  F_2305154  F_2512253  F_2737957  \\\n",
       "count      35.000000  ...  35.000000  35.000000  35.000000  35.000000   \n",
       "mean   116727.257143  ... -76.877714 -75.893714 -74.838286 -73.658571   \n",
       "std     15899.351476  ...   0.582485   0.625493   0.672819   0.726418   \n",
       "min     88926.000000  ... -77.510000 -76.550000 -75.520000 -74.370000   \n",
       "25%    106051.500000  ... -77.220000 -76.260000 -75.230000 -74.085000   \n",
       "50%    114883.000000  ... -77.140000 -76.180000 -75.140000 -73.970000   \n",
       "75%    128092.000000  ... -76.775000 -75.800000 -74.760000 -73.600000   \n",
       "max    148136.000000  ... -75.280000 -74.180000 -73.010000 -71.690000   \n",
       "\n",
       "       F_2983939  F_3252021  F_3544187  F_3862602  F_4209624  F_4587823  \n",
       "count  35.000000  35.000000  35.000000  35.000000  35.000000  35.000000  \n",
       "mean  -72.259714 -70.768571 -69.070000 -67.214857 -65.264571 -63.398286  \n",
       "std     0.788817   0.877416   0.952341   1.034236   1.160645   1.410057  \n",
       "min   -73.010000 -71.570000 -69.930000 -68.120000 -66.280000 -64.680000  \n",
       "25%   -72.710000 -71.270000 -69.610000 -67.810000 -65.950000 -64.295000  \n",
       "50%   -72.620000 -71.180000 -69.520000 -67.690000 -65.800000 -64.010000  \n",
       "75%   -72.240000 -70.785000 -69.050000 -67.220000 -65.295000 -63.335000  \n",
       "max   -70.100000 -68.430000 -66.490000 -64.380000 -62.240000 -60.420000  \n",
       "\n",
       "[8 rows x 77 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dados básicos - TESTE\n",
    "dataset_teste.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2dc0a251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    29\n",
       "1.0     6\n",
       "Name: G_SAÚDE, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_teste['G_SAÚDE'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3594a3",
   "metadata": {},
   "source": [
    "### 3 - Pré-Processamento de dados: Separação em conjunto de treino e conjunto de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7d3768cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separação em conjuntos de treino e teste\n",
    "\n",
    "#test_size = 0.1\n",
    "#seed = 7\n",
    "#X = array_treino [:,2:79].astype(float)\n",
    "#Y = array_treino [:,1].astype(int)\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "array_treino = dataset_treino.values\n",
    "array_teste = dataset_teste.values\n",
    "\n",
    "X_train = array_treino [:,4:79].astype(float)\n",
    "X_test = array_teste [:,4:79].astype(float)\n",
    "\n",
    "Y_train =  array_treino [:,1].astype(int)\n",
    "Y_test =  array_teste [:,1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "71fe43d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 74)\n",
      "(35, 74)\n",
      "(84,)\n",
      "(35,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f9679945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.75</td>\n",
       "      <td>156335.0</td>\n",
       "      <td>143802.0</td>\n",
       "      <td>132243.0</td>\n",
       "      <td>121611.0</td>\n",
       "      <td>111818.0</td>\n",
       "      <td>102827.0</td>\n",
       "      <td>94542.0</td>\n",
       "      <td>86935.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.11</td>\n",
       "      <td>-76.14</td>\n",
       "      <td>-75.10</td>\n",
       "      <td>-73.95</td>\n",
       "      <td>-72.59</td>\n",
       "      <td>-71.16</td>\n",
       "      <td>-69.51</td>\n",
       "      <td>-67.70</td>\n",
       "      <td>-65.79</td>\n",
       "      <td>-64.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>160660.0</td>\n",
       "      <td>147742.0</td>\n",
       "      <td>135860.0</td>\n",
       "      <td>124897.0</td>\n",
       "      <td>114802.0</td>\n",
       "      <td>105567.0</td>\n",
       "      <td>97018.0</td>\n",
       "      <td>89269.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-75.96</td>\n",
       "      <td>-74.88</td>\n",
       "      <td>-73.72</td>\n",
       "      <td>-72.41</td>\n",
       "      <td>-70.87</td>\n",
       "      <td>-69.16</td>\n",
       "      <td>-67.32</td>\n",
       "      <td>-65.28</td>\n",
       "      <td>-63.04</td>\n",
       "      <td>-60.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>197720.0</td>\n",
       "      <td>181686.0</td>\n",
       "      <td>167597.0</td>\n",
       "      <td>154044.0</td>\n",
       "      <td>141841.0</td>\n",
       "      <td>130338.0</td>\n",
       "      <td>119989.0</td>\n",
       "      <td>110238.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-76.01</td>\n",
       "      <td>-74.98</td>\n",
       "      <td>-73.86</td>\n",
       "      <td>-72.61</td>\n",
       "      <td>-71.08</td>\n",
       "      <td>-69.42</td>\n",
       "      <td>-67.60</td>\n",
       "      <td>-65.62</td>\n",
       "      <td>-63.43</td>\n",
       "      <td>-60.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>193522.0</td>\n",
       "      <td>177844.0</td>\n",
       "      <td>163527.0</td>\n",
       "      <td>150331.0</td>\n",
       "      <td>138195.0</td>\n",
       "      <td>127087.0</td>\n",
       "      <td>116826.0</td>\n",
       "      <td>107400.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.31</td>\n",
       "      <td>-76.35</td>\n",
       "      <td>-75.33</td>\n",
       "      <td>-74.18</td>\n",
       "      <td>-72.82</td>\n",
       "      <td>-71.38</td>\n",
       "      <td>-69.72</td>\n",
       "      <td>-67.92</td>\n",
       "      <td>-66.05</td>\n",
       "      <td>-64.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.75</td>\n",
       "      <td>134048.0</td>\n",
       "      <td>123242.0</td>\n",
       "      <td>113351.0</td>\n",
       "      <td>104234.0</td>\n",
       "      <td>95843.0</td>\n",
       "      <td>88112.0</td>\n",
       "      <td>81027.0</td>\n",
       "      <td>74503.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.22</td>\n",
       "      <td>-76.25</td>\n",
       "      <td>-75.20</td>\n",
       "      <td>-74.04</td>\n",
       "      <td>-72.70</td>\n",
       "      <td>-71.25</td>\n",
       "      <td>-69.61</td>\n",
       "      <td>-67.81</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>-64.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>146452.0</td>\n",
       "      <td>134718.0</td>\n",
       "      <td>123921.0</td>\n",
       "      <td>113932.0</td>\n",
       "      <td>104755.0</td>\n",
       "      <td>96268.0</td>\n",
       "      <td>88539.0</td>\n",
       "      <td>81396.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-75.94</td>\n",
       "      <td>-74.86</td>\n",
       "      <td>-73.68</td>\n",
       "      <td>-72.38</td>\n",
       "      <td>-70.83</td>\n",
       "      <td>-69.15</td>\n",
       "      <td>-67.30</td>\n",
       "      <td>-65.26</td>\n",
       "      <td>-63.01</td>\n",
       "      <td>-60.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>152141.0</td>\n",
       "      <td>139912.0</td>\n",
       "      <td>128677.0</td>\n",
       "      <td>118295.0</td>\n",
       "      <td>108789.0</td>\n",
       "      <td>100042.0</td>\n",
       "      <td>91971.0</td>\n",
       "      <td>84559.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.12</td>\n",
       "      <td>-76.16</td>\n",
       "      <td>-75.11</td>\n",
       "      <td>-73.95</td>\n",
       "      <td>-72.60</td>\n",
       "      <td>-71.16</td>\n",
       "      <td>-69.50</td>\n",
       "      <td>-67.69</td>\n",
       "      <td>-65.80</td>\n",
       "      <td>-64.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>190207.0</td>\n",
       "      <td>175174.0</td>\n",
       "      <td>161269.0</td>\n",
       "      <td>148257.0</td>\n",
       "      <td>136533.0</td>\n",
       "      <td>125656.0</td>\n",
       "      <td>115652.0</td>\n",
       "      <td>106372.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-76.83</td>\n",
       "      <td>-75.91</td>\n",
       "      <td>-74.93</td>\n",
       "      <td>-73.89</td>\n",
       "      <td>-72.48</td>\n",
       "      <td>-71.10</td>\n",
       "      <td>-69.25</td>\n",
       "      <td>-67.49</td>\n",
       "      <td>-65.70</td>\n",
       "      <td>-64.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>205727.0</td>\n",
       "      <td>188942.0</td>\n",
       "      <td>174012.0</td>\n",
       "      <td>160102.0</td>\n",
       "      <td>147293.0</td>\n",
       "      <td>135414.0</td>\n",
       "      <td>124522.0</td>\n",
       "      <td>114560.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-76.02</td>\n",
       "      <td>-74.99</td>\n",
       "      <td>-73.90</td>\n",
       "      <td>-72.64</td>\n",
       "      <td>-71.13</td>\n",
       "      <td>-69.44</td>\n",
       "      <td>-67.62</td>\n",
       "      <td>-65.64</td>\n",
       "      <td>-63.45</td>\n",
       "      <td>-61.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>188039.0</td>\n",
       "      <td>172877.0</td>\n",
       "      <td>158891.0</td>\n",
       "      <td>146043.0</td>\n",
       "      <td>134217.0</td>\n",
       "      <td>123375.0</td>\n",
       "      <td>113390.0</td>\n",
       "      <td>104203.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.56</td>\n",
       "      <td>-76.60</td>\n",
       "      <td>-75.58</td>\n",
       "      <td>-74.43</td>\n",
       "      <td>-73.07</td>\n",
       "      <td>-71.64</td>\n",
       "      <td>-69.98</td>\n",
       "      <td>-68.17</td>\n",
       "      <td>-66.32</td>\n",
       "      <td>-64.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1         2         3         4         5         6         7   \\\n",
       "0   1.0  1.75  156335.0  143802.0  132243.0  121611.0  111818.0  102827.0   \n",
       "1   0.5  0.25  160660.0  147742.0  135860.0  124897.0  114802.0  105567.0   \n",
       "2   2.0  1.25  197720.0  181686.0  167597.0  154044.0  141841.0  130338.0   \n",
       "3   1.5  0.75  193522.0  177844.0  163527.0  150331.0  138195.0  127087.0   \n",
       "4   1.0  2.75  134048.0  123242.0  113351.0  104234.0   95843.0   88112.0   \n",
       "..  ...   ...       ...       ...       ...       ...       ...       ...   \n",
       "79  0.5  0.75  146452.0  134718.0  123921.0  113932.0  104755.0   96268.0   \n",
       "80  1.0  2.25  152141.0  139912.0  128677.0  118295.0  108789.0  100042.0   \n",
       "81  2.0  0.25  190207.0  175174.0  161269.0  148257.0  136533.0  125656.0   \n",
       "82  2.0  1.00  205727.0  188942.0  174012.0  160102.0  147293.0  135414.0   \n",
       "83  1.5  0.75  188039.0  172877.0  158891.0  146043.0  134217.0  123375.0   \n",
       "\n",
       "          8         9   ...     64     65     66     67     68     69     70  \\\n",
       "0    94542.0   86935.0  ... -77.11 -76.14 -75.10 -73.95 -72.59 -71.16 -69.51   \n",
       "1    97018.0   89269.0  ... -75.96 -74.88 -73.72 -72.41 -70.87 -69.16 -67.32   \n",
       "2   119989.0  110238.0  ... -76.01 -74.98 -73.86 -72.61 -71.08 -69.42 -67.60   \n",
       "3   116826.0  107400.0  ... -77.31 -76.35 -75.33 -74.18 -72.82 -71.38 -69.72   \n",
       "4    81027.0   74503.0  ... -77.22 -76.25 -75.20 -74.04 -72.70 -71.25 -69.61   \n",
       "..       ...       ...  ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "79   88539.0   81396.0  ... -75.94 -74.86 -73.68 -72.38 -70.83 -69.15 -67.30   \n",
       "80   91971.0   84559.0  ... -77.12 -76.16 -75.11 -73.95 -72.60 -71.16 -69.50   \n",
       "81  115652.0  106372.0  ... -76.83 -75.91 -74.93 -73.89 -72.48 -71.10 -69.25   \n",
       "82  124522.0  114560.0  ... -76.02 -74.99 -73.90 -72.64 -71.13 -69.44 -67.62   \n",
       "83  113390.0  104203.0  ... -77.56 -76.60 -75.58 -74.43 -73.07 -71.64 -69.98   \n",
       "\n",
       "       71     72     73  \n",
       "0  -67.70 -65.79 -64.02  \n",
       "1  -65.28 -63.04 -60.51  \n",
       "2  -65.62 -63.43 -60.98  \n",
       "3  -67.92 -66.05 -64.39  \n",
       "4  -67.81 -65.91 -64.05  \n",
       "..    ...    ...    ...  \n",
       "79 -65.26 -63.01 -60.49  \n",
       "80 -67.69 -65.80 -64.01  \n",
       "81 -67.49 -65.70 -64.05  \n",
       "82 -65.64 -63.45 -61.04  \n",
       "83 -68.17 -66.32 -64.66  \n",
       "\n",
       "[84 rows x 74 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6c41d88e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.75</td>\n",
       "      <td>148575.0</td>\n",
       "      <td>136711.0</td>\n",
       "      <td>125671.0</td>\n",
       "      <td>115588.0</td>\n",
       "      <td>106287.0</td>\n",
       "      <td>97725.0</td>\n",
       "      <td>89862.0</td>\n",
       "      <td>82627.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.14</td>\n",
       "      <td>-76.18</td>\n",
       "      <td>-75.14</td>\n",
       "      <td>-73.97</td>\n",
       "      <td>-72.62</td>\n",
       "      <td>-71.18</td>\n",
       "      <td>-69.53</td>\n",
       "      <td>-67.71</td>\n",
       "      <td>-65.80</td>\n",
       "      <td>-64.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.5</td>\n",
       "      <td>2.25</td>\n",
       "      <td>167194.0</td>\n",
       "      <td>153726.0</td>\n",
       "      <td>141303.0</td>\n",
       "      <td>129867.0</td>\n",
       "      <td>119368.0</td>\n",
       "      <td>109710.0</td>\n",
       "      <td>100818.0</td>\n",
       "      <td>92672.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.51</td>\n",
       "      <td>-76.55</td>\n",
       "      <td>-75.52</td>\n",
       "      <td>-74.37</td>\n",
       "      <td>-73.01</td>\n",
       "      <td>-71.57</td>\n",
       "      <td>-69.93</td>\n",
       "      <td>-68.12</td>\n",
       "      <td>-66.24</td>\n",
       "      <td>-64.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>143487.0</td>\n",
       "      <td>132062.0</td>\n",
       "      <td>121487.0</td>\n",
       "      <td>111750.0</td>\n",
       "      <td>102804.0</td>\n",
       "      <td>94566.0</td>\n",
       "      <td>87001.0</td>\n",
       "      <td>80027.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-75.79</td>\n",
       "      <td>-74.73</td>\n",
       "      <td>-73.58</td>\n",
       "      <td>-72.31</td>\n",
       "      <td>-70.81</td>\n",
       "      <td>-69.14</td>\n",
       "      <td>-67.32</td>\n",
       "      <td>-65.32</td>\n",
       "      <td>-63.12</td>\n",
       "      <td>-60.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>174040.0</td>\n",
       "      <td>160035.0</td>\n",
       "      <td>147157.0</td>\n",
       "      <td>135301.0</td>\n",
       "      <td>124406.0</td>\n",
       "      <td>114394.0</td>\n",
       "      <td>105175.0</td>\n",
       "      <td>96650.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.31</td>\n",
       "      <td>-76.36</td>\n",
       "      <td>-75.35</td>\n",
       "      <td>-74.21</td>\n",
       "      <td>-72.82</td>\n",
       "      <td>-71.39</td>\n",
       "      <td>-69.73</td>\n",
       "      <td>-67.90</td>\n",
       "      <td>-66.04</td>\n",
       "      <td>-64.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2.75</td>\n",
       "      <td>141582.0</td>\n",
       "      <td>130170.0</td>\n",
       "      <td>119727.0</td>\n",
       "      <td>110060.0</td>\n",
       "      <td>101257.0</td>\n",
       "      <td>93137.0</td>\n",
       "      <td>85663.0</td>\n",
       "      <td>78785.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-75.28</td>\n",
       "      <td>-74.18</td>\n",
       "      <td>-73.01</td>\n",
       "      <td>-71.69</td>\n",
       "      <td>-70.10</td>\n",
       "      <td>-68.43</td>\n",
       "      <td>-66.49</td>\n",
       "      <td>-64.38</td>\n",
       "      <td>-62.24</td>\n",
       "      <td>-60.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2.25</td>\n",
       "      <td>125071.0</td>\n",
       "      <td>115057.0</td>\n",
       "      <td>105836.0</td>\n",
       "      <td>97374.0</td>\n",
       "      <td>89568.0</td>\n",
       "      <td>82398.0</td>\n",
       "      <td>75767.0</td>\n",
       "      <td>69690.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-76.82</td>\n",
       "      <td>-75.85</td>\n",
       "      <td>-74.83</td>\n",
       "      <td>-73.72</td>\n",
       "      <td>-72.31</td>\n",
       "      <td>-70.85</td>\n",
       "      <td>-69.09</td>\n",
       "      <td>-67.27</td>\n",
       "      <td>-65.36</td>\n",
       "      <td>-63.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>159674.0</td>\n",
       "      <td>146857.0</td>\n",
       "      <td>135159.0</td>\n",
       "      <td>124276.0</td>\n",
       "      <td>114280.0</td>\n",
       "      <td>105113.0</td>\n",
       "      <td>96673.0</td>\n",
       "      <td>88898.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.02</td>\n",
       "      <td>-76.06</td>\n",
       "      <td>-75.03</td>\n",
       "      <td>-73.88</td>\n",
       "      <td>-72.52</td>\n",
       "      <td>-71.08</td>\n",
       "      <td>-69.43</td>\n",
       "      <td>-67.62</td>\n",
       "      <td>-65.73</td>\n",
       "      <td>-63.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>160626.0</td>\n",
       "      <td>147502.0</td>\n",
       "      <td>135850.0</td>\n",
       "      <td>125048.0</td>\n",
       "      <td>114883.0</td>\n",
       "      <td>105652.0</td>\n",
       "      <td>97170.0</td>\n",
       "      <td>89373.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-76.09</td>\n",
       "      <td>-75.03</td>\n",
       "      <td>-73.90</td>\n",
       "      <td>-72.64</td>\n",
       "      <td>-71.14</td>\n",
       "      <td>-69.46</td>\n",
       "      <td>-67.67</td>\n",
       "      <td>-65.69</td>\n",
       "      <td>-63.49</td>\n",
       "      <td>-61.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>147963.0</td>\n",
       "      <td>136066.0</td>\n",
       "      <td>125137.0</td>\n",
       "      <td>115045.0</td>\n",
       "      <td>105816.0</td>\n",
       "      <td>97292.0</td>\n",
       "      <td>89445.0</td>\n",
       "      <td>82246.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.17</td>\n",
       "      <td>-76.20</td>\n",
       "      <td>-75.15</td>\n",
       "      <td>-73.99</td>\n",
       "      <td>-72.64</td>\n",
       "      <td>-71.20</td>\n",
       "      <td>-69.56</td>\n",
       "      <td>-67.75</td>\n",
       "      <td>-65.85</td>\n",
       "      <td>-64.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.75</td>\n",
       "      <td>159978.0</td>\n",
       "      <td>147116.0</td>\n",
       "      <td>135439.0</td>\n",
       "      <td>124490.0</td>\n",
       "      <td>114459.0</td>\n",
       "      <td>105267.0</td>\n",
       "      <td>96768.0</td>\n",
       "      <td>88986.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.11</td>\n",
       "      <td>-76.14</td>\n",
       "      <td>-75.11</td>\n",
       "      <td>-73.95</td>\n",
       "      <td>-72.59</td>\n",
       "      <td>-71.15</td>\n",
       "      <td>-69.50</td>\n",
       "      <td>-67.69</td>\n",
       "      <td>-65.80</td>\n",
       "      <td>-64.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.5</td>\n",
       "      <td>2.75</td>\n",
       "      <td>164269.0</td>\n",
       "      <td>151047.0</td>\n",
       "      <td>138904.0</td>\n",
       "      <td>127722.0</td>\n",
       "      <td>117429.0</td>\n",
       "      <td>107973.0</td>\n",
       "      <td>99268.0</td>\n",
       "      <td>91261.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.18</td>\n",
       "      <td>-76.21</td>\n",
       "      <td>-75.18</td>\n",
       "      <td>-74.02</td>\n",
       "      <td>-72.66</td>\n",
       "      <td>-71.22</td>\n",
       "      <td>-69.58</td>\n",
       "      <td>-67.77</td>\n",
       "      <td>-65.88</td>\n",
       "      <td>-64.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>183945.0</td>\n",
       "      <td>169079.0</td>\n",
       "      <td>155598.0</td>\n",
       "      <td>143021.0</td>\n",
       "      <td>131512.0</td>\n",
       "      <td>120930.0</td>\n",
       "      <td>111207.0</td>\n",
       "      <td>102233.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.18</td>\n",
       "      <td>-76.22</td>\n",
       "      <td>-75.19</td>\n",
       "      <td>-74.04</td>\n",
       "      <td>-72.66</td>\n",
       "      <td>-71.21</td>\n",
       "      <td>-69.52</td>\n",
       "      <td>-67.73</td>\n",
       "      <td>-65.86</td>\n",
       "      <td>-64.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.5</td>\n",
       "      <td>1.25</td>\n",
       "      <td>183358.0</td>\n",
       "      <td>168572.0</td>\n",
       "      <td>154993.0</td>\n",
       "      <td>142564.0</td>\n",
       "      <td>131061.0</td>\n",
       "      <td>120544.0</td>\n",
       "      <td>110817.0</td>\n",
       "      <td>101883.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.21</td>\n",
       "      <td>-76.25</td>\n",
       "      <td>-75.23</td>\n",
       "      <td>-74.09</td>\n",
       "      <td>-72.72</td>\n",
       "      <td>-71.28</td>\n",
       "      <td>-69.61</td>\n",
       "      <td>-67.82</td>\n",
       "      <td>-65.95</td>\n",
       "      <td>-64.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.75</td>\n",
       "      <td>151410.0</td>\n",
       "      <td>139266.0</td>\n",
       "      <td>128138.0</td>\n",
       "      <td>117860.0</td>\n",
       "      <td>108428.0</td>\n",
       "      <td>99713.0</td>\n",
       "      <td>91721.0</td>\n",
       "      <td>84366.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-76.91</td>\n",
       "      <td>-75.95</td>\n",
       "      <td>-74.92</td>\n",
       "      <td>-73.77</td>\n",
       "      <td>-72.41</td>\n",
       "      <td>-70.97</td>\n",
       "      <td>-69.32</td>\n",
       "      <td>-67.51</td>\n",
       "      <td>-65.62</td>\n",
       "      <td>-63.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2.75</td>\n",
       "      <td>124558.0</td>\n",
       "      <td>114572.0</td>\n",
       "      <td>105401.0</td>\n",
       "      <td>96991.0</td>\n",
       "      <td>89230.0</td>\n",
       "      <td>82080.0</td>\n",
       "      <td>75498.0</td>\n",
       "      <td>69454.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-76.73</td>\n",
       "      <td>-75.75</td>\n",
       "      <td>-74.70</td>\n",
       "      <td>-73.54</td>\n",
       "      <td>-72.17</td>\n",
       "      <td>-70.72</td>\n",
       "      <td>-69.01</td>\n",
       "      <td>-67.17</td>\n",
       "      <td>-65.23</td>\n",
       "      <td>-63.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>174863.0</td>\n",
       "      <td>160834.0</td>\n",
       "      <td>147978.0</td>\n",
       "      <td>136013.0</td>\n",
       "      <td>125123.0</td>\n",
       "      <td>115075.0</td>\n",
       "      <td>105828.0</td>\n",
       "      <td>97308.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.07</td>\n",
       "      <td>-76.12</td>\n",
       "      <td>-75.09</td>\n",
       "      <td>-73.94</td>\n",
       "      <td>-72.59</td>\n",
       "      <td>-71.15</td>\n",
       "      <td>-69.51</td>\n",
       "      <td>-67.69</td>\n",
       "      <td>-65.82</td>\n",
       "      <td>-64.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>187798.0</td>\n",
       "      <td>172803.0</td>\n",
       "      <td>159313.0</td>\n",
       "      <td>146332.0</td>\n",
       "      <td>134484.0</td>\n",
       "      <td>123840.0</td>\n",
       "      <td>113859.0</td>\n",
       "      <td>104768.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-76.06</td>\n",
       "      <td>-75.02</td>\n",
       "      <td>-73.90</td>\n",
       "      <td>-72.63</td>\n",
       "      <td>-71.12</td>\n",
       "      <td>-69.45</td>\n",
       "      <td>-67.63</td>\n",
       "      <td>-65.66</td>\n",
       "      <td>-63.46</td>\n",
       "      <td>-61.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>158721.0</td>\n",
       "      <td>145978.0</td>\n",
       "      <td>134206.0</td>\n",
       "      <td>123402.0</td>\n",
       "      <td>113505.0</td>\n",
       "      <td>104396.0</td>\n",
       "      <td>95960.0</td>\n",
       "      <td>88292.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-76.01</td>\n",
       "      <td>-74.95</td>\n",
       "      <td>-73.81</td>\n",
       "      <td>-72.53</td>\n",
       "      <td>-71.02</td>\n",
       "      <td>-69.35</td>\n",
       "      <td>-67.54</td>\n",
       "      <td>-65.55</td>\n",
       "      <td>-63.34</td>\n",
       "      <td>-60.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.5</td>\n",
       "      <td>1.75</td>\n",
       "      <td>168010.0</td>\n",
       "      <td>154543.0</td>\n",
       "      <td>142028.0</td>\n",
       "      <td>130520.0</td>\n",
       "      <td>120021.0</td>\n",
       "      <td>110326.0</td>\n",
       "      <td>101410.0</td>\n",
       "      <td>93210.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.46</td>\n",
       "      <td>-76.50</td>\n",
       "      <td>-75.47</td>\n",
       "      <td>-74.32</td>\n",
       "      <td>-72.96</td>\n",
       "      <td>-71.52</td>\n",
       "      <td>-69.87</td>\n",
       "      <td>-68.08</td>\n",
       "      <td>-66.20</td>\n",
       "      <td>-64.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.5</td>\n",
       "      <td>1.75</td>\n",
       "      <td>151682.0</td>\n",
       "      <td>139450.0</td>\n",
       "      <td>128230.0</td>\n",
       "      <td>117895.0</td>\n",
       "      <td>108371.0</td>\n",
       "      <td>99667.0</td>\n",
       "      <td>91609.0</td>\n",
       "      <td>84218.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.31</td>\n",
       "      <td>-76.36</td>\n",
       "      <td>-75.33</td>\n",
       "      <td>-74.17</td>\n",
       "      <td>-72.80</td>\n",
       "      <td>-71.36</td>\n",
       "      <td>-69.72</td>\n",
       "      <td>-67.90</td>\n",
       "      <td>-65.99</td>\n",
       "      <td>-64.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.75</td>\n",
       "      <td>141512.0</td>\n",
       "      <td>130208.0</td>\n",
       "      <td>119767.0</td>\n",
       "      <td>110199.0</td>\n",
       "      <td>101379.0</td>\n",
       "      <td>93250.0</td>\n",
       "      <td>85774.0</td>\n",
       "      <td>78892.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-76.82</td>\n",
       "      <td>-75.86</td>\n",
       "      <td>-74.82</td>\n",
       "      <td>-73.66</td>\n",
       "      <td>-72.32</td>\n",
       "      <td>-70.87</td>\n",
       "      <td>-69.24</td>\n",
       "      <td>-67.42</td>\n",
       "      <td>-65.52</td>\n",
       "      <td>-63.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>142645.0</td>\n",
       "      <td>131248.0</td>\n",
       "      <td>120732.0</td>\n",
       "      <td>111043.0</td>\n",
       "      <td>102104.0</td>\n",
       "      <td>93879.0</td>\n",
       "      <td>86316.0</td>\n",
       "      <td>79364.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-76.00</td>\n",
       "      <td>-74.94</td>\n",
       "      <td>-73.79</td>\n",
       "      <td>-72.51</td>\n",
       "      <td>-71.01</td>\n",
       "      <td>-69.35</td>\n",
       "      <td>-67.53</td>\n",
       "      <td>-65.53</td>\n",
       "      <td>-63.31</td>\n",
       "      <td>-60.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>200667.0</td>\n",
       "      <td>184480.0</td>\n",
       "      <td>169699.0</td>\n",
       "      <td>156120.0</td>\n",
       "      <td>143553.0</td>\n",
       "      <td>132026.0</td>\n",
       "      <td>121436.0</td>\n",
       "      <td>111676.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.16</td>\n",
       "      <td>-76.20</td>\n",
       "      <td>-75.19</td>\n",
       "      <td>-74.05</td>\n",
       "      <td>-72.69</td>\n",
       "      <td>-71.25</td>\n",
       "      <td>-69.61</td>\n",
       "      <td>-67.80</td>\n",
       "      <td>-65.95</td>\n",
       "      <td>-64.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>152932.0</td>\n",
       "      <td>140461.0</td>\n",
       "      <td>129264.0</td>\n",
       "      <td>118824.0</td>\n",
       "      <td>109261.0</td>\n",
       "      <td>100426.0</td>\n",
       "      <td>92357.0</td>\n",
       "      <td>84906.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.09</td>\n",
       "      <td>-76.12</td>\n",
       "      <td>-75.07</td>\n",
       "      <td>-73.91</td>\n",
       "      <td>-72.55</td>\n",
       "      <td>-71.10</td>\n",
       "      <td>-69.43</td>\n",
       "      <td>-67.62</td>\n",
       "      <td>-65.72</td>\n",
       "      <td>-63.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>187999.0</td>\n",
       "      <td>172845.0</td>\n",
       "      <td>158948.0</td>\n",
       "      <td>146147.0</td>\n",
       "      <td>134383.0</td>\n",
       "      <td>123572.0</td>\n",
       "      <td>113608.0</td>\n",
       "      <td>104432.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.41</td>\n",
       "      <td>-76.46</td>\n",
       "      <td>-75.46</td>\n",
       "      <td>-74.33</td>\n",
       "      <td>-72.95</td>\n",
       "      <td>-71.52</td>\n",
       "      <td>-69.83</td>\n",
       "      <td>-68.04</td>\n",
       "      <td>-66.24</td>\n",
       "      <td>-64.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>207257.0</td>\n",
       "      <td>190447.0</td>\n",
       "      <td>175266.0</td>\n",
       "      <td>161116.0</td>\n",
       "      <td>148136.0</td>\n",
       "      <td>136210.0</td>\n",
       "      <td>125202.0</td>\n",
       "      <td>115141.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.29</td>\n",
       "      <td>-76.34</td>\n",
       "      <td>-75.31</td>\n",
       "      <td>-74.17</td>\n",
       "      <td>-72.80</td>\n",
       "      <td>-71.36</td>\n",
       "      <td>-69.70</td>\n",
       "      <td>-67.87</td>\n",
       "      <td>-66.01</td>\n",
       "      <td>-64.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.25</td>\n",
       "      <td>156041.0</td>\n",
       "      <td>143481.0</td>\n",
       "      <td>131857.0</td>\n",
       "      <td>121263.0</td>\n",
       "      <td>111473.0</td>\n",
       "      <td>102459.0</td>\n",
       "      <td>94170.0</td>\n",
       "      <td>86562.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.21</td>\n",
       "      <td>-76.23</td>\n",
       "      <td>-75.18</td>\n",
       "      <td>-74.01</td>\n",
       "      <td>-72.64</td>\n",
       "      <td>-71.18</td>\n",
       "      <td>-69.52</td>\n",
       "      <td>-67.69</td>\n",
       "      <td>-65.78</td>\n",
       "      <td>-63.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>165007.0</td>\n",
       "      <td>151714.0</td>\n",
       "      <td>139517.0</td>\n",
       "      <td>128308.0</td>\n",
       "      <td>118004.0</td>\n",
       "      <td>108533.0</td>\n",
       "      <td>99803.0</td>\n",
       "      <td>91779.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.04</td>\n",
       "      <td>-76.07</td>\n",
       "      <td>-75.05</td>\n",
       "      <td>-73.89</td>\n",
       "      <td>-72.54</td>\n",
       "      <td>-71.09</td>\n",
       "      <td>-69.44</td>\n",
       "      <td>-67.62</td>\n",
       "      <td>-65.74</td>\n",
       "      <td>-64.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>171585.0</td>\n",
       "      <td>157955.0</td>\n",
       "      <td>145088.0</td>\n",
       "      <td>133528.0</td>\n",
       "      <td>122793.0</td>\n",
       "      <td>112939.0</td>\n",
       "      <td>103846.0</td>\n",
       "      <td>95523.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-76.19</td>\n",
       "      <td>-75.16</td>\n",
       "      <td>-74.04</td>\n",
       "      <td>-72.80</td>\n",
       "      <td>-71.33</td>\n",
       "      <td>-69.79</td>\n",
       "      <td>-67.97</td>\n",
       "      <td>-66.04</td>\n",
       "      <td>-64.00</td>\n",
       "      <td>-62.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>203625.0</td>\n",
       "      <td>187208.0</td>\n",
       "      <td>172081.0</td>\n",
       "      <td>158220.0</td>\n",
       "      <td>145432.0</td>\n",
       "      <td>133756.0</td>\n",
       "      <td>122916.0</td>\n",
       "      <td>112999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.44</td>\n",
       "      <td>-76.50</td>\n",
       "      <td>-75.49</td>\n",
       "      <td>-74.36</td>\n",
       "      <td>-73.00</td>\n",
       "      <td>-71.57</td>\n",
       "      <td>-69.91</td>\n",
       "      <td>-68.12</td>\n",
       "      <td>-66.28</td>\n",
       "      <td>-64.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.5</td>\n",
       "      <td>1.25</td>\n",
       "      <td>183991.0</td>\n",
       "      <td>169221.0</td>\n",
       "      <td>155548.0</td>\n",
       "      <td>143039.0</td>\n",
       "      <td>131477.0</td>\n",
       "      <td>120884.0</td>\n",
       "      <td>111157.0</td>\n",
       "      <td>102192.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.21</td>\n",
       "      <td>-76.26</td>\n",
       "      <td>-75.23</td>\n",
       "      <td>-74.08</td>\n",
       "      <td>-72.70</td>\n",
       "      <td>-71.26</td>\n",
       "      <td>-69.60</td>\n",
       "      <td>-67.80</td>\n",
       "      <td>-65.93</td>\n",
       "      <td>-64.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>124299.0</td>\n",
       "      <td>114338.0</td>\n",
       "      <td>105136.0</td>\n",
       "      <td>96720.0</td>\n",
       "      <td>88926.0</td>\n",
       "      <td>81797.0</td>\n",
       "      <td>75194.0</td>\n",
       "      <td>69168.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-75.76</td>\n",
       "      <td>-74.68</td>\n",
       "      <td>-73.53</td>\n",
       "      <td>-72.23</td>\n",
       "      <td>-70.71</td>\n",
       "      <td>-69.04</td>\n",
       "      <td>-67.22</td>\n",
       "      <td>-65.21</td>\n",
       "      <td>-62.98</td>\n",
       "      <td>-60.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>195872.0</td>\n",
       "      <td>180151.0</td>\n",
       "      <td>165706.0</td>\n",
       "      <td>152360.0</td>\n",
       "      <td>140038.0</td>\n",
       "      <td>128761.0</td>\n",
       "      <td>118370.0</td>\n",
       "      <td>108788.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.34</td>\n",
       "      <td>-76.38</td>\n",
       "      <td>-75.37</td>\n",
       "      <td>-74.22</td>\n",
       "      <td>-72.86</td>\n",
       "      <td>-71.42</td>\n",
       "      <td>-69.77</td>\n",
       "      <td>-67.95</td>\n",
       "      <td>-66.09</td>\n",
       "      <td>-64.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.5</td>\n",
       "      <td>2.25</td>\n",
       "      <td>168181.0</td>\n",
       "      <td>154685.0</td>\n",
       "      <td>142208.0</td>\n",
       "      <td>130733.0</td>\n",
       "      <td>120215.0</td>\n",
       "      <td>110529.0</td>\n",
       "      <td>101639.0</td>\n",
       "      <td>93442.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.17</td>\n",
       "      <td>-76.21</td>\n",
       "      <td>-75.17</td>\n",
       "      <td>-74.02</td>\n",
       "      <td>-72.65</td>\n",
       "      <td>-71.21</td>\n",
       "      <td>-69.56</td>\n",
       "      <td>-67.74</td>\n",
       "      <td>-65.86</td>\n",
       "      <td>-64.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2.75</td>\n",
       "      <td>134444.0</td>\n",
       "      <td>123512.0</td>\n",
       "      <td>113591.0</td>\n",
       "      <td>104404.0</td>\n",
       "      <td>95988.0</td>\n",
       "      <td>88236.0</td>\n",
       "      <td>81114.0</td>\n",
       "      <td>74555.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.23</td>\n",
       "      <td>-76.26</td>\n",
       "      <td>-75.20</td>\n",
       "      <td>-74.03</td>\n",
       "      <td>-72.67</td>\n",
       "      <td>-71.21</td>\n",
       "      <td>-69.56</td>\n",
       "      <td>-67.74</td>\n",
       "      <td>-65.83</td>\n",
       "      <td>-63.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1         2         3         4         5         6         7   \\\n",
       "0   1.0  2.75  148575.0  136711.0  125671.0  115588.0  106287.0   97725.0   \n",
       "1   1.5  2.25  167194.0  153726.0  141303.0  129867.0  119368.0  109710.0   \n",
       "2   1.0  2.00  143487.0  132062.0  121487.0  111750.0  102804.0   94566.0   \n",
       "3   1.5  0.75  174040.0  160035.0  147157.0  135301.0  124406.0  114394.0   \n",
       "4   0.5  2.75  141582.0  130170.0  119727.0  110060.0  101257.0   93137.0   \n",
       "5   0.5  2.25  125071.0  115057.0  105836.0   97374.0   89568.0   82398.0   \n",
       "6   1.0  1.25  159674.0  146857.0  135159.0  124276.0  114280.0  105113.0   \n",
       "7   2.0  2.50  160626.0  147502.0  135850.0  125048.0  114883.0  105652.0   \n",
       "8   1.0  2.25  147963.0  136066.0  125137.0  115045.0  105816.0   97292.0   \n",
       "9   1.0  1.75  159978.0  147116.0  135439.0  124490.0  114459.0  105267.0   \n",
       "10  1.5  2.75  164269.0  151047.0  138904.0  127722.0  117429.0  107973.0   \n",
       "11  1.0  0.75  183945.0  169079.0  155598.0  143021.0  131512.0  120930.0   \n",
       "12  1.5  1.25  183358.0  168572.0  154993.0  142564.0  131061.0  120544.0   \n",
       "13  1.0  1.75  151410.0  139266.0  128138.0  117860.0  108428.0   99713.0   \n",
       "14  0.5  2.75  124558.0  114572.0  105401.0   96991.0   89230.0   82080.0   \n",
       "15  1.0  0.75  174863.0  160834.0  147978.0  136013.0  125123.0  115075.0   \n",
       "16  2.0  1.50  187798.0  172803.0  159313.0  146332.0  134484.0  123840.0   \n",
       "17  1.5  1.00  158721.0  145978.0  134206.0  123402.0  113505.0  104396.0   \n",
       "18  1.5  1.75  168010.0  154543.0  142028.0  130520.0  120021.0  110326.0   \n",
       "19  1.5  1.75  151682.0  139450.0  128230.0  117895.0  108371.0   99667.0   \n",
       "20  1.0  2.75  141512.0  130208.0  119767.0  110199.0  101379.0   93250.0   \n",
       "21  1.5  2.50  142645.0  131248.0  120732.0  111043.0  102104.0   93879.0   \n",
       "22  1.0  0.25  200667.0  184480.0  169699.0  156120.0  143553.0  132026.0   \n",
       "23  1.0  2.25  152932.0  140461.0  129264.0  118824.0  109261.0  100426.0   \n",
       "24  1.5  0.25  187999.0  172845.0  158948.0  146147.0  134383.0  123572.0   \n",
       "25  1.0  0.25  207257.0  190447.0  175266.0  161116.0  148136.0  136210.0   \n",
       "26  0.5  1.25  156041.0  143481.0  131857.0  121263.0  111473.0  102459.0   \n",
       "27  1.0  1.25  165007.0  151714.0  139517.0  128308.0  118004.0  108533.0   \n",
       "28  0.5  0.75  171585.0  157955.0  145088.0  133528.0  122793.0  112939.0   \n",
       "29  1.5  0.25  203625.0  187208.0  172081.0  158220.0  145432.0  133756.0   \n",
       "30  1.5  1.25  183991.0  169221.0  155548.0  143039.0  131477.0  120884.0   \n",
       "31  0.5  2.50  124299.0  114338.0  105136.0   96720.0   88926.0   81797.0   \n",
       "32  1.5  0.75  195872.0  180151.0  165706.0  152360.0  140038.0  128761.0   \n",
       "33  1.5  2.25  168181.0  154685.0  142208.0  130733.0  120215.0  110529.0   \n",
       "34  0.5  2.75  134444.0  123512.0  113591.0  104404.0   95988.0   88236.0   \n",
       "\n",
       "          8         9   ...     64     65     66     67     68     69     70  \\\n",
       "0    89862.0   82627.0  ... -77.14 -76.18 -75.14 -73.97 -72.62 -71.18 -69.53   \n",
       "1   100818.0   92672.0  ... -77.51 -76.55 -75.52 -74.37 -73.01 -71.57 -69.93   \n",
       "2    87001.0   80027.0  ... -75.79 -74.73 -73.58 -72.31 -70.81 -69.14 -67.32   \n",
       "3   105175.0   96650.0  ... -77.31 -76.36 -75.35 -74.21 -72.82 -71.39 -69.73   \n",
       "4    85663.0   78785.0  ... -75.28 -74.18 -73.01 -71.69 -70.10 -68.43 -66.49   \n",
       "5    75767.0   69690.0  ... -76.82 -75.85 -74.83 -73.72 -72.31 -70.85 -69.09   \n",
       "6    96673.0   88898.0  ... -77.02 -76.06 -75.03 -73.88 -72.52 -71.08 -69.43   \n",
       "7    97170.0   89373.0  ... -76.09 -75.03 -73.90 -72.64 -71.14 -69.46 -67.67   \n",
       "8    89445.0   82246.0  ... -77.17 -76.20 -75.15 -73.99 -72.64 -71.20 -69.56   \n",
       "9    96768.0   88986.0  ... -77.11 -76.14 -75.11 -73.95 -72.59 -71.15 -69.50   \n",
       "10   99268.0   91261.0  ... -77.18 -76.21 -75.18 -74.02 -72.66 -71.22 -69.58   \n",
       "11  111207.0  102233.0  ... -77.18 -76.22 -75.19 -74.04 -72.66 -71.21 -69.52   \n",
       "12  110817.0  101883.0  ... -77.21 -76.25 -75.23 -74.09 -72.72 -71.28 -69.61   \n",
       "13   91721.0   84366.0  ... -76.91 -75.95 -74.92 -73.77 -72.41 -70.97 -69.32   \n",
       "14   75498.0   69454.0  ... -76.73 -75.75 -74.70 -73.54 -72.17 -70.72 -69.01   \n",
       "15  105828.0   97308.0  ... -77.07 -76.12 -75.09 -73.94 -72.59 -71.15 -69.51   \n",
       "16  113859.0  104768.0  ... -76.06 -75.02 -73.90 -72.63 -71.12 -69.45 -67.63   \n",
       "17   95960.0   88292.0  ... -76.01 -74.95 -73.81 -72.53 -71.02 -69.35 -67.54   \n",
       "18  101410.0   93210.0  ... -77.46 -76.50 -75.47 -74.32 -72.96 -71.52 -69.87   \n",
       "19   91609.0   84218.0  ... -77.31 -76.36 -75.33 -74.17 -72.80 -71.36 -69.72   \n",
       "20   85774.0   78892.0  ... -76.82 -75.86 -74.82 -73.66 -72.32 -70.87 -69.24   \n",
       "21   86316.0   79364.0  ... -76.00 -74.94 -73.79 -72.51 -71.01 -69.35 -67.53   \n",
       "22  121436.0  111676.0  ... -77.16 -76.20 -75.19 -74.05 -72.69 -71.25 -69.61   \n",
       "23   92357.0   84906.0  ... -77.09 -76.12 -75.07 -73.91 -72.55 -71.10 -69.43   \n",
       "24  113608.0  104432.0  ... -77.41 -76.46 -75.46 -74.33 -72.95 -71.52 -69.83   \n",
       "25  125202.0  115141.0  ... -77.29 -76.34 -75.31 -74.17 -72.80 -71.36 -69.70   \n",
       "26   94170.0   86562.0  ... -77.21 -76.23 -75.18 -74.01 -72.64 -71.18 -69.52   \n",
       "27   99803.0   91779.0  ... -77.04 -76.07 -75.05 -73.89 -72.54 -71.09 -69.44   \n",
       "28  103846.0   95523.0  ... -76.19 -75.16 -74.04 -72.80 -71.33 -69.79 -67.97   \n",
       "29  122916.0  112999.0  ... -77.44 -76.50 -75.49 -74.36 -73.00 -71.57 -69.91   \n",
       "30  111157.0  102192.0  ... -77.21 -76.26 -75.23 -74.08 -72.70 -71.26 -69.60   \n",
       "31   75194.0   69168.0  ... -75.76 -74.68 -73.53 -72.23 -70.71 -69.04 -67.22   \n",
       "32  118370.0  108788.0  ... -77.34 -76.38 -75.37 -74.22 -72.86 -71.42 -69.77   \n",
       "33  101639.0   93442.0  ... -77.17 -76.21 -75.17 -74.02 -72.65 -71.21 -69.56   \n",
       "34   81114.0   74555.0  ... -77.23 -76.26 -75.20 -74.03 -72.67 -71.21 -69.56   \n",
       "\n",
       "       71     72     73  \n",
       "0  -67.71 -65.80 -64.01  \n",
       "1  -68.12 -66.24 -64.52  \n",
       "2  -65.32 -63.12 -60.65  \n",
       "3  -67.90 -66.04 -64.38  \n",
       "4  -64.38 -62.24 -60.42  \n",
       "5  -67.27 -65.36 -63.45  \n",
       "6  -67.62 -65.73 -63.99  \n",
       "7  -65.69 -63.49 -61.05  \n",
       "8  -67.75 -65.85 -64.04  \n",
       "9  -67.69 -65.80 -64.04  \n",
       "10 -67.77 -65.88 -64.14  \n",
       "11 -67.73 -65.86 -64.17  \n",
       "12 -67.82 -65.95 -64.29  \n",
       "13 -67.51 -65.62 -63.84  \n",
       "14 -67.17 -65.23 -63.22  \n",
       "15 -67.69 -65.82 -64.10  \n",
       "16 -65.66 -63.46 -61.02  \n",
       "17 -65.55 -63.34 -60.89  \n",
       "18 -68.08 -66.20 -64.48  \n",
       "19 -67.90 -65.99 -64.30  \n",
       "20 -67.42 -65.52 -63.70  \n",
       "21 -65.53 -63.31 -60.87  \n",
       "22 -67.80 -65.95 -64.35  \n",
       "23 -67.62 -65.72 -63.93  \n",
       "24 -68.04 -66.24 -64.60  \n",
       "25 -67.87 -66.01 -64.42  \n",
       "26 -67.69 -65.78 -63.98  \n",
       "27 -67.62 -65.74 -64.00  \n",
       "28 -66.04 -64.00 -62.14  \n",
       "29 -68.12 -66.28 -64.68  \n",
       "30 -67.80 -65.93 -64.24  \n",
       "31 -65.21 -62.98 -60.48  \n",
       "32 -67.95 -66.09 -64.47  \n",
       "33 -67.74 -65.86 -64.13  \n",
       "34 -67.74 -65.83 -63.95  \n",
       "\n",
       "[35 rows x 74 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ee37b996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0\n",
       "0   0\n",
       "1   1\n",
       "2   1\n",
       "3   0\n",
       "4   0\n",
       ".. ..\n",
       "79  1\n",
       "80  0\n",
       "81  0\n",
       "82  1\n",
       "83  0\n",
       "\n",
       "[84 rows x 1 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cf090740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0\n",
       "0   0\n",
       "1   0\n",
       "2   1\n",
       "3   0\n",
       "4   0\n",
       "5   0\n",
       "6   0\n",
       "7   1\n",
       "8   0\n",
       "9   0\n",
       "10  0\n",
       "11  0\n",
       "12  0\n",
       "13  0\n",
       "14  0\n",
       "15  0\n",
       "16  1\n",
       "17  1\n",
       "18  0\n",
       "19  0\n",
       "20  0\n",
       "21  1\n",
       "22  0\n",
       "23  0\n",
       "24  0\n",
       "25  0\n",
       "26  0\n",
       "27  0\n",
       "28  0\n",
       "29  0\n",
       "30  0\n",
       "31  1\n",
       "32  0\n",
       "33  0\n",
       "34  0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04389dae",
   "metadata": {},
   "source": [
    "### 4 - Ajuste dos modelos  - validação de melhores parametros (model tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c121b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros\n",
    "num_folds = 10\n",
    "scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7be225",
   "metadata": {},
   "source": [
    "##### 4.1 - Ajuste do modelo KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9c8dd0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor: 0.976389 usando {'metric': 'euclidean', 'n_neighbors': 1}\n",
      "0.976389 (0.047324): {'metric': 'euclidean', 'n_neighbors': 1}\n",
      "0.976389 (0.047324): {'metric': 'euclidean', 'n_neighbors': 3}\n",
      "0.963889 (0.078617): {'metric': 'euclidean', 'n_neighbors': 5}\n",
      "0.929167 (0.080615): {'metric': 'euclidean', 'n_neighbors': 7}\n",
      "0.906944 (0.101313): {'metric': 'euclidean', 'n_neighbors': 9}\n",
      "0.929167 (0.080615): {'metric': 'euclidean', 'n_neighbors': 11}\n",
      "0.905556 (0.088672): {'metric': 'euclidean', 'n_neighbors': 13}\n",
      "0.905556 (0.088672): {'metric': 'euclidean', 'n_neighbors': 15}\n",
      "0.905556 (0.088672): {'metric': 'euclidean', 'n_neighbors': 17}\n",
      "0.905556 (0.088672): {'metric': 'euclidean', 'n_neighbors': 19}\n",
      "0.881944 (0.106002): {'metric': 'euclidean', 'n_neighbors': 21}\n",
      "0.965278 (0.072768): {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "0.929167 (0.094699): {'metric': 'manhattan', 'n_neighbors': 3}\n",
      "0.929167 (0.080615): {'metric': 'manhattan', 'n_neighbors': 5}\n",
      "0.929167 (0.080615): {'metric': 'manhattan', 'n_neighbors': 7}\n",
      "0.918056 (0.092223): {'metric': 'manhattan', 'n_neighbors': 9}\n",
      "0.929167 (0.080615): {'metric': 'manhattan', 'n_neighbors': 11}\n",
      "0.916667 (0.078322): {'metric': 'manhattan', 'n_neighbors': 13}\n",
      "0.916667 (0.078322): {'metric': 'manhattan', 'n_neighbors': 15}\n",
      "0.918056 (0.092223): {'metric': 'manhattan', 'n_neighbors': 17}\n",
      "0.893056 (0.100164): {'metric': 'manhattan', 'n_neighbors': 19}\n",
      "0.904167 (0.092640): {'metric': 'manhattan', 'n_neighbors': 21}\n",
      "0.976389 (0.047324): {'metric': 'minkowski', 'n_neighbors': 1}\n",
      "0.976389 (0.047324): {'metric': 'minkowski', 'n_neighbors': 3}\n",
      "0.963889 (0.078617): {'metric': 'minkowski', 'n_neighbors': 5}\n",
      "0.929167 (0.080615): {'metric': 'minkowski', 'n_neighbors': 7}\n",
      "0.906944 (0.101313): {'metric': 'minkowski', 'n_neighbors': 9}\n",
      "0.929167 (0.080615): {'metric': 'minkowski', 'n_neighbors': 11}\n",
      "0.905556 (0.088672): {'metric': 'minkowski', 'n_neighbors': 13}\n",
      "0.905556 (0.088672): {'metric': 'minkowski', 'n_neighbors': 15}\n",
      "0.905556 (0.088672): {'metric': 'minkowski', 'n_neighbors': 17}\n",
      "0.905556 (0.088672): {'metric': 'minkowski', 'n_neighbors': 19}\n",
      "0.881944 (0.106002): {'metric': 'minkowski', 'n_neighbors': 21}\n"
     ]
    }
   ],
   "source": [
    "# definindo uma semente global\n",
    "np.random.seed(7)\n",
    "\n",
    "# Tuning do KNN\n",
    "\n",
    "# Padroniza dados\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "\n",
    "# Definir os hiperparâmetros para ajustar\n",
    "#==#\n",
    "# Principal parametro. Define o número de vizinhos.\n",
    "k = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]\n",
    "# Determina o algoritmo de calculo a ser utilizado\n",
    "distancias = ['euclidean', 'manhattan', 'minkowski']\n",
    "#\n",
    "param_grid = dict(n_neighbors=k, \n",
    "                  metric=distancias)\n",
    "\n",
    "# Modelo de treinamento definido\n",
    "model = KNeighborsClassifier()\n",
    "# Número de divisões - folds\n",
    "kfold = KFold(n_splits=num_folds)\n",
    "\n",
    "# Criar o objeto GridSearchCV\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print('Melhor: %f usando %s' %(grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print('%f (%f): %r' % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c549cf73",
   "metadata": {},
   "source": [
    "##### 4.2 - Ajuste do modelo SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9f43fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor: 0.988889 com {'C': 0.1, 'kernel': 'linear'}\n",
      "0.988889 (0.033333): {'C': 0.1, 'kernel': 'linear'}\n",
      "0.920833 (0.165511): {'C': 0.1, 'kernel': 'poly'}\n",
      "0.906944 (0.130445): {'C': 0.1, 'kernel': 'rbf'}\n",
      "0.931944 (0.133860): {'C': 0.1, 'kernel': 'sigmoid'}\n",
      "0.988889 (0.033333): {'C': 0.5, 'kernel': 'linear'}\n",
      "0.920833 (0.165511): {'C': 0.5, 'kernel': 'poly'}\n",
      "0.930556 (0.134371): {'C': 0.5, 'kernel': 'rbf'}\n",
      "0.869444 (0.150642): {'C': 0.5, 'kernel': 'sigmoid'}\n",
      "0.988889 (0.033333): {'C': 1.0, 'kernel': 'linear'}\n",
      "0.931944 (0.166719): {'C': 1.0, 'kernel': 'poly'}\n",
      "0.943056 (0.134436): {'C': 1.0, 'kernel': 'rbf'}\n",
      "0.880556 (0.129725): {'C': 1.0, 'kernel': 'sigmoid'}\n",
      "0.988889 (0.033333): {'C': 1.5, 'kernel': 'linear'}\n",
      "0.931944 (0.133860): {'C': 1.5, 'kernel': 'poly'}\n",
      "0.943056 (0.134436): {'C': 1.5, 'kernel': 'rbf'}\n",
      "0.893056 (0.134522): {'C': 1.5, 'kernel': 'sigmoid'}\n",
      "0.988889 (0.033333): {'C': 2.0, 'kernel': 'linear'}\n",
      "0.931944 (0.133860): {'C': 2.0, 'kernel': 'poly'}\n",
      "0.943056 (0.134436): {'C': 2.0, 'kernel': 'rbf'}\n",
      "0.905556 (0.128770): {'C': 2.0, 'kernel': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "# definindo uma semente global\n",
    "np.random.seed(7) \n",
    "\n",
    "# Tuning do SVM\n",
    "\n",
    "# Padroniza dados\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "\n",
    "# Definir os hiperparâmetros para ajustar\n",
    "#==#\n",
    "# Significa o valor de regularização (inversamente proporcional ao grau de regularização)\n",
    "c_values = [0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "# Significa a selecção do kernel para o algoritmo de aprendizagem automática SVM.\n",
    "kernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "#\n",
    "param_grid = dict(C=c_values, \n",
    "                  kernel=kernel_values)\n",
    "\n",
    "# Modelo de treinamento definido\n",
    "model = SVC()\n",
    "# Número de divisões - folds\n",
    "kfold = KFold(n_splits=num_folds)\n",
    "\n",
    "# Criar o objeto GridSearchCV\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print('Melhor: %f com %s' % (grid_result.best_score_,grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print('%f (%f): %r' % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831a4b23",
   "metadata": {},
   "source": [
    "##### 4.3 - Ajuste do modelo Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab953b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor: 1.000000 com {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "0.976389 (0.047324): {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.988889 (0.033333): {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.976389 (0.047324): {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.965278 (0.072768): {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.988889 (0.033333): {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.976389 (0.047324): {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "0.988889 (0.033333): {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
      "0.988889 (0.033333): {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 5}\n",
      "0.988889 (0.033333): {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 10}\n",
      "0.976389 (0.047324): {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.976389 (0.047324): {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.988889 (0.033333): {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.965278 (0.072768): {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.976389 (0.047324): {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.988889 (0.033333): {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "0.976389 (0.047324): {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
      "0.965278 (0.072768): {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 3, 'min_samples_split': 5}\n",
      "0.988889 (0.033333): {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 3, 'min_samples_split': 10}\n",
      "0.988889 (0.033333): {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.988889 (0.033333): {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.987500 (0.037500): {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.976389 (0.047324): {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.976389 (0.047324): {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.976389 (0.047324): {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "0.976389 (0.047324): {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
      "0.976389 (0.047324): {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 5}\n",
      "0.988889 (0.033333): {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 10}\n",
      "0.988889 (0.033333): {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.976389 (0.047324): {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.976389 (0.047324): {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.988889 (0.033333): {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.976389 (0.047324): {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.987500 (0.037500): {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "0.976389 (0.047324): {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
      "0.988889 (0.033333): {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 3, 'min_samples_split': 5}\n",
      "0.988889 (0.033333): {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 3, 'min_samples_split': 10}\n",
      "0.976389 (0.047324): {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.977778 (0.066667): {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.976389 (0.047324): {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.976389 (0.047324): {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.987500 (0.037500): {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.976389 (0.047324): {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "0.988889 (0.033333): {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
      "0.976389 (0.047324): {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 5}\n",
      "0.988889 (0.033333): {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 10}\n",
      "0.987500 (0.037500): {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.988889 (0.033333): {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.976389 (0.047324): {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.976389 (0.047324): {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.976389 (0.047324): {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "1.000000 (0.000000): {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "0.976389 (0.047324): {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
      "0.976389 (0.047324): {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 3, 'min_samples_split': 5}\n",
      "0.988889 (0.033333): {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 3, 'min_samples_split': 10}\n",
      "0.988889 (0.033333): {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.987500 (0.037500): {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "1.000000 (0.000000): {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.976389 (0.047324): {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.976389 (0.047324): {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.976389 (0.047324): {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "0.977778 (0.066667): {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
      "0.976389 (0.047324): {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 5}\n",
      "1.000000 (0.000000): {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 10}\n",
      "0.976389 (0.047324): {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.988889 (0.033333): {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.977778 (0.066667): {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.988889 (0.033333): {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.976389 (0.047324): {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.976389 (0.047324): {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "0.976389 (0.047324): {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
      "0.976389 (0.047324): {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 3, 'min_samples_split': 5}\n",
      "0.988889 (0.033333): {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 3, 'min_samples_split': 10}\n",
      "0.976389 (0.047324): {'criterion': 'log_loss', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.988889 (0.033333): {'criterion': 'log_loss', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.987500 (0.037500): {'criterion': 'log_loss', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.988889 (0.033333): {'criterion': 'log_loss', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.988889 (0.033333): {'criterion': 'log_loss', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.976389 (0.047324): {'criterion': 'log_loss', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "0.976389 (0.047324): {'criterion': 'log_loss', 'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
      "0.988889 (0.033333): {'criterion': 'log_loss', 'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 5}\n",
      "0.988889 (0.033333): {'criterion': 'log_loss', 'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 10}\n",
      "0.976389 (0.047324): {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.976389 (0.047324): {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.977778 (0.066667): {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.988889 (0.033333): {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.988889 (0.033333): {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.988889 (0.033333): {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "0.988889 (0.033333): {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
      "0.987500 (0.037500): {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_leaf': 3, 'min_samples_split': 5}\n",
      "0.976389 (0.047324): {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_leaf': 3, 'min_samples_split': 10}\n",
      "0.976389 (0.047324): {'criterion': 'log_loss', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.976389 (0.047324): {'criterion': 'log_loss', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.976389 (0.047324): {'criterion': 'log_loss', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.965278 (0.072768): {'criterion': 'log_loss', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.988889 (0.033333): {'criterion': 'log_loss', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.976389 (0.047324): {'criterion': 'log_loss', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "0.976389 (0.047324): {'criterion': 'log_loss', 'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
      "0.987500 (0.037500): {'criterion': 'log_loss', 'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 5}\n",
      "0.976389 (0.047324): {'criterion': 'log_loss', 'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 10}\n",
      "0.976389 (0.047324): {'criterion': 'log_loss', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.987500 (0.037500): {'criterion': 'log_loss', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.988889 (0.033333): {'criterion': 'log_loss', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.987500 (0.037500): {'criterion': 'log_loss', 'max_depth': 15, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.976389 (0.047324): {'criterion': 'log_loss', 'max_depth': 15, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.976389 (0.047324): {'criterion': 'log_loss', 'max_depth': 15, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "0.976389 (0.047324): {'criterion': 'log_loss', 'max_depth': 15, 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
      "0.976389 (0.047324): {'criterion': 'log_loss', 'max_depth': 15, 'min_samples_leaf': 3, 'min_samples_split': 5}\n",
      "1.000000 (0.000000): {'criterion': 'log_loss', 'max_depth': 15, 'min_samples_leaf': 3, 'min_samples_split': 10}\n"
     ]
    }
   ],
   "source": [
    "# definindo uma semente global\n",
    "np.random.seed(7) \n",
    "\n",
    "# Tuning do Decision Tree Classifier\n",
    "\n",
    "# Padroniza dados\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "\n",
    "# Definir os hiperparâmetros para ajustar\n",
    "#==#\n",
    "# Criterio de divisão da árvore de decisão\n",
    "criterion_values = ['gini', 'entropy', 'log_loss']\n",
    "# Profundidade máxima da árvore\n",
    "max_depth_values = [None, 5, 10, 15]\n",
    "# Número mínimo de amostras para dividir um nó interno\n",
    "min_samples_split_values = [2, 5, 10]\n",
    "# Número mínimo de amostras em um nó folha\n",
    "min_samples_leaf_values = [1, 2, 3]\n",
    "# Agrupamento dos hiperparâmetros\n",
    "#\n",
    "param_grid = dict(criterion = criterion_values, \n",
    "                  max_depth = max_depth_values, \n",
    "                  min_samples_split = min_samples_split_values, \n",
    "                  min_samples_leaf = min_samples_leaf_values)\n",
    "\n",
    "\n",
    "# Modelo de treinamento definido\n",
    "model = DecisionTreeClassifier()\n",
    "# Número de divisões - folds\n",
    "kfold = KFold(n_splits=num_folds)\n",
    "\n",
    "                  \n",
    "# Criar o objeto GridSearchCV\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print('Melhor: %f com %s' % (grid_result.best_score_,grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print('%f (%f): %r' % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3789cc44",
   "metadata": {},
   "source": [
    "##### 4.4 - Ajuste do modelo GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "109088c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor: 0.965278 com {'var_smoothing': 5e-05}\n",
      "0.943056 (0.134436): {'var_smoothing': 0.05}\n",
      "0.965278 (0.072768): {'var_smoothing': 5e-05}\n",
      "0.965278 (0.072768): {'var_smoothing': 5e-08}\n"
     ]
    }
   ],
   "source": [
    "# definindo uma semente global\n",
    "np.random.seed(7) \n",
    "\n",
    "# Tuning do SVM\n",
    "\n",
    "# Padroniza dados\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "\n",
    "# Definir os hiperparâmetros para ajustar\n",
    "#==#\n",
    "# Parametro de suavização da variância do cálculo da estabilidade\n",
    "var_smoothing_values = [5e-2, 5e-5, 5e-8]\n",
    "\n",
    "param_grid = dict(var_smoothing = var_smoothing_values)\n",
    "\n",
    "# Modelo de treinamento definido\n",
    "model = GaussianNB()\n",
    "# Número de divisões - folds\n",
    "kfold = KFold(n_splits=num_folds)\n",
    "\n",
    "# Criar o objeto GridSearchCV\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print('Melhor: %f com %s' % (grid_result.best_score_,grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print('%f (%f): %r' % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a983ccb",
   "metadata": {},
   "source": [
    "##### 4.5 - Ajuste do modelo Regressão Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfbc26a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor: 0.988889 com {'C': 0.01, 'penalty': None, 'solver': 'lbfgs'}\n",
      "0.988889 (0.033333): {'C': 0.01, 'penalty': None, 'solver': 'lbfgs'}\n",
      "0.988889 (0.033333): {'C': 0.01, 'penalty': None, 'solver': 'newton-cg'}\n",
      "nan (nan): {'C': 0.01, 'penalty': None, 'solver': 'liblinear'}\n",
      "0.988889 (0.033333): {'C': 0.01, 'penalty': None, 'solver': 'newton-cg'}\n",
      "0.966667 (0.071146): {'C': 0.01, 'penalty': None, 'solver': 'newton-cholesky'}\n",
      "0.988889 (0.033333): {'C': 0.01, 'penalty': None, 'solver': 'sag'}\n",
      "0.977778 (0.066667): {'C': 0.01, 'penalty': None, 'solver': 'saga'}\n",
      "nan (nan): {'C': 0.01, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "nan (nan): {'C': 0.01, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "0.495833 (0.137500): {'C': 0.01, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "nan (nan): {'C': 0.01, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "nan (nan): {'C': 0.01, 'penalty': 'l1', 'solver': 'newton-cholesky'}\n",
      "nan (nan): {'C': 0.01, 'penalty': 'l1', 'solver': 'sag'}\n",
      "0.506944 (0.137388): {'C': 0.01, 'penalty': 'l1', 'solver': 'saga'}\n",
      "nan (nan): {'C': 0.01, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
      "nan (nan): {'C': 0.01, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "nan (nan): {'C': 0.01, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
      "nan (nan): {'C': 0.01, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "nan (nan): {'C': 0.01, 'penalty': 'elasticnet', 'solver': 'newton-cholesky'}\n",
      "nan (nan): {'C': 0.01, 'penalty': 'elasticnet', 'solver': 'sag'}\n",
      "nan (nan): {'C': 0.01, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
      "0.930556 (0.134371): {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.930556 (0.134371): {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.931944 (0.133860): {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.930556 (0.134371): {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.930556 (0.134371): {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cholesky'}\n",
      "0.930556 (0.134371): {'C': 0.01, 'penalty': 'l2', 'solver': 'sag'}\n",
      "0.930556 (0.134371): {'C': 0.01, 'penalty': 'l2', 'solver': 'saga'}\n",
      "nan (nan): {'C': 0.01, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
      "nan (nan): {'C': 0.01, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "nan (nan): {'C': 0.01, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
      "nan (nan): {'C': 0.01, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "nan (nan): {'C': 0.01, 'penalty': 'elasticnet', 'solver': 'newton-cholesky'}\n",
      "nan (nan): {'C': 0.01, 'penalty': 'elasticnet', 'solver': 'sag'}\n",
      "nan (nan): {'C': 0.01, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
      "0.988889 (0.033333): {'C': 0.1, 'penalty': None, 'solver': 'lbfgs'}\n",
      "0.988889 (0.033333): {'C': 0.1, 'penalty': None, 'solver': 'newton-cg'}\n",
      "nan (nan): {'C': 0.1, 'penalty': None, 'solver': 'liblinear'}\n",
      "0.988889 (0.033333): {'C': 0.1, 'penalty': None, 'solver': 'newton-cg'}\n",
      "0.966667 (0.071146): {'C': 0.1, 'penalty': None, 'solver': 'newton-cholesky'}\n",
      "0.988889 (0.033333): {'C': 0.1, 'penalty': None, 'solver': 'sag'}\n",
      "0.977778 (0.066667): {'C': 0.1, 'penalty': None, 'solver': 'saga'}\n",
      "nan (nan): {'C': 0.1, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "nan (nan): {'C': 0.1, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "0.977778 (0.066667): {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "nan (nan): {'C': 0.1, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "nan (nan): {'C': 0.1, 'penalty': 'l1', 'solver': 'newton-cholesky'}\n",
      "nan (nan): {'C': 0.1, 'penalty': 'l1', 'solver': 'sag'}\n",
      "0.943056 (0.134436): {'C': 0.1, 'penalty': 'l1', 'solver': 'saga'}\n",
      "nan (nan): {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
      "nan (nan): {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "nan (nan): {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
      "nan (nan): {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "nan (nan): {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'newton-cholesky'}\n",
      "nan (nan): {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'sag'}\n",
      "nan (nan): {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
      "0.943056 (0.134436): {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.943056 (0.134436): {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.943056 (0.134436): {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.943056 (0.134436): {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.943056 (0.134436): {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cholesky'}\n",
      "0.943056 (0.134436): {'C': 0.1, 'penalty': 'l2', 'solver': 'sag'}\n",
      "0.943056 (0.134436): {'C': 0.1, 'penalty': 'l2', 'solver': 'saga'}\n",
      "nan (nan): {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
      "nan (nan): {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "nan (nan): {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
      "nan (nan): {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "nan (nan): {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'newton-cholesky'}\n",
      "nan (nan): {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'sag'}\n",
      "nan (nan): {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
      "0.988889 (0.033333): {'C': 1, 'penalty': None, 'solver': 'lbfgs'}\n",
      "0.988889 (0.033333): {'C': 1, 'penalty': None, 'solver': 'newton-cg'}\n",
      "nan (nan): {'C': 1, 'penalty': None, 'solver': 'liblinear'}\n",
      "0.988889 (0.033333): {'C': 1, 'penalty': None, 'solver': 'newton-cg'}\n",
      "0.966667 (0.071146): {'C': 1, 'penalty': None, 'solver': 'newton-cholesky'}\n",
      "0.988889 (0.033333): {'C': 1, 'penalty': None, 'solver': 'sag'}\n",
      "0.977778 (0.066667): {'C': 1, 'penalty': None, 'solver': 'saga'}\n",
      "nan (nan): {'C': 1, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "nan (nan): {'C': 1, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "0.977778 (0.066667): {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "nan (nan): {'C': 1, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "nan (nan): {'C': 1, 'penalty': 'l1', 'solver': 'newton-cholesky'}\n",
      "nan (nan): {'C': 1, 'penalty': 'l1', 'solver': 'sag'}\n",
      "0.966667 (0.100000): {'C': 1, 'penalty': 'l1', 'solver': 'saga'}\n",
      "nan (nan): {'C': 1, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
      "nan (nan): {'C': 1, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "nan (nan): {'C': 1, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
      "nan (nan): {'C': 1, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "nan (nan): {'C': 1, 'penalty': 'elasticnet', 'solver': 'newton-cholesky'}\n",
      "nan (nan): {'C': 1, 'penalty': 'elasticnet', 'solver': 'sag'}\n",
      "nan (nan): {'C': 1, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
      "0.988889 (0.033333): {'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.988889 (0.033333): {'C': 1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.977778 (0.066667): {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.988889 (0.033333): {'C': 1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.988889 (0.033333): {'C': 1, 'penalty': 'l2', 'solver': 'newton-cholesky'}\n",
      "0.977778 (0.066667): {'C': 1, 'penalty': 'l2', 'solver': 'sag'}\n",
      "0.977778 (0.066667): {'C': 1, 'penalty': 'l2', 'solver': 'saga'}\n",
      "nan (nan): {'C': 1, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
      "nan (nan): {'C': 1, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "nan (nan): {'C': 1, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
      "nan (nan): {'C': 1, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "nan (nan): {'C': 1, 'penalty': 'elasticnet', 'solver': 'newton-cholesky'}\n",
      "nan (nan): {'C': 1, 'penalty': 'elasticnet', 'solver': 'sag'}\n",
      "nan (nan): {'C': 1, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
      "0.988889 (0.033333): {'C': 10, 'penalty': None, 'solver': 'lbfgs'}\n",
      "0.988889 (0.033333): {'C': 10, 'penalty': None, 'solver': 'newton-cg'}\n",
      "nan (nan): {'C': 10, 'penalty': None, 'solver': 'liblinear'}\n",
      "0.988889 (0.033333): {'C': 10, 'penalty': None, 'solver': 'newton-cg'}\n",
      "0.966667 (0.071146): {'C': 10, 'penalty': None, 'solver': 'newton-cholesky'}\n",
      "0.988889 (0.033333): {'C': 10, 'penalty': None, 'solver': 'sag'}\n",
      "0.977778 (0.066667): {'C': 10, 'penalty': None, 'solver': 'saga'}\n",
      "nan (nan): {'C': 10, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "nan (nan): {'C': 10, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "0.988889 (0.033333): {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "nan (nan): {'C': 10, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "nan (nan): {'C': 10, 'penalty': 'l1', 'solver': 'newton-cholesky'}\n",
      "nan (nan): {'C': 10, 'penalty': 'l1', 'solver': 'sag'}\n",
      "0.977778 (0.066667): {'C': 10, 'penalty': 'l1', 'solver': 'saga'}\n",
      "nan (nan): {'C': 10, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
      "nan (nan): {'C': 10, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "nan (nan): {'C': 10, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
      "nan (nan): {'C': 10, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "nan (nan): {'C': 10, 'penalty': 'elasticnet', 'solver': 'newton-cholesky'}\n",
      "nan (nan): {'C': 10, 'penalty': 'elasticnet', 'solver': 'sag'}\n",
      "nan (nan): {'C': 10, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
      "0.988889 (0.033333): {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.988889 (0.033333): {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.988889 (0.033333): {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.988889 (0.033333): {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.988889 (0.033333): {'C': 10, 'penalty': 'l2', 'solver': 'newton-cholesky'}\n",
      "0.988889 (0.033333): {'C': 10, 'penalty': 'l2', 'solver': 'sag'}\n",
      "0.977778 (0.066667): {'C': 10, 'penalty': 'l2', 'solver': 'saga'}\n",
      "nan (nan): {'C': 10, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
      "nan (nan): {'C': 10, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "nan (nan): {'C': 10, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
      "nan (nan): {'C': 10, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "nan (nan): {'C': 10, 'penalty': 'elasticnet', 'solver': 'newton-cholesky'}\n",
      "nan (nan): {'C': 10, 'penalty': 'elasticnet', 'solver': 'sag'}\n",
      "nan (nan): {'C': 10, 'penalty': 'elasticnet', 'solver': 'saga'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "800 fits failed out of a total of 1400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1216, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "160 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1048, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 864, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 782, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.98888889 0.98888889        nan 0.98888889 0.96666667 0.98888889\n",
      " 0.97777778        nan        nan 0.49583333        nan        nan\n",
      "        nan 0.50694444        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.93055556 0.93055556 0.93194444\n",
      " 0.93055556 0.93055556 0.93055556 0.93055556        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.98888889\n",
      " 0.98888889        nan 0.98888889 0.96666667 0.98888889 0.97777778\n",
      "        nan        nan 0.97777778        nan        nan        nan\n",
      " 0.94305556        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.94305556 0.94305556 0.94305556 0.94305556\n",
      " 0.94305556 0.94305556 0.94305556        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.98888889 0.98888889\n",
      "        nan 0.98888889 0.96666667 0.98888889 0.97777778        nan\n",
      "        nan 0.97777778        nan        nan        nan 0.96666667\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.98888889 0.98888889 0.97777778 0.98888889 0.98888889\n",
      " 0.97777778 0.97777778        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.98888889 0.98888889        nan\n",
      " 0.98888889 0.96666667 0.98888889 0.97777778        nan        nan\n",
      " 0.98888889        nan        nan        nan 0.97777778        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.98888889 0.98888889 0.98888889 0.98888889 0.98888889 0.98888889\n",
      " 0.97777778        nan        nan        nan        nan        nan\n",
      "        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# definindo uma semente global\n",
    "np.random.seed(7) \n",
    "\n",
    "# Tuning do SVM\n",
    "\n",
    "# Padroniza dados\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "\n",
    "# Definir os hiperparâmetros para ajustar\n",
    "#==#\n",
    "# Significa o valor de regularização (inversamente proporcional ao grau de regularização)\n",
    "c_values = [0.01,0.1, 1, 10]\n",
    "# Especifica o valor de penalidade\n",
    "penalty_values = [None, 'l1', 'elasticnet', 'l2', 'elasticnet']\n",
    "# Algoritmo a utilizar no problema de optimização\n",
    "solver_values = ['lbfgs', 'newton-cg', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
    "#\n",
    "param_grid = dict(C = c_values, \n",
    "                  penalty = penalty_values,\n",
    "                 solver = solver_values)\n",
    "\n",
    "# Modelo de treinamento definido\n",
    "model = LogisticRegression()\n",
    "# Número de divisões - folds\n",
    "kfold = KFold(n_splits=num_folds)\n",
    "\n",
    "# Criar o objeto GridSearchCV\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print('Melhor: %f com %s' % (grid_result.best_score_,grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print('%f (%f): %r' % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd814df",
   "metadata": {},
   "source": [
    "#### 5 - Modelos de Classificação, com os parametros que tiveram melhores resultados no teste de alto ajuste (model tuning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2ceb91",
   "metadata": {},
   "source": [
    "#### 5.1 - Criação e avaliação de modelos: base sem tratamento  (sem normalização dos dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50f2a8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros\n",
    "num_folds = 10\n",
    "scoring = 'accuracy'\n",
    "\n",
    "# Criação dos modelos\n",
    "models = []\n",
    "\n",
    "models.append(('LR', LogisticRegression(C= 0.01,\n",
    "                                        penalty= None,\n",
    "                                        solver= 'lbfgs')))\n",
    "\n",
    "models.append(('KNN', KNeighborsClassifier(metric = 'euclidean',\n",
    "                                           n_neighbors = 1)))\n",
    "\n",
    "models.append(('CART', DecisionTreeClassifier(criterion = 'gini',\n",
    "                                              max_depth = 10,\n",
    "                                              min_samples_leaf = 1,\n",
    "                                              min_samples_split = 5)))\n",
    "\n",
    "                \n",
    "models.append(('NB', GaussianNB(var_smoothing = 5e-05)))\n",
    "\n",
    "models.append(('SVM', SVC(C = 0.1,\n",
    "                          kernel = 'linear')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b00b02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.987500 (0.037500)\n",
      "KNN: 0.491667 (0.114058)\n",
      "CART: 0.976389 (0.047324)\n",
      "NB: 0.623611 (0.202840)\n",
      "SVM: 1.000000 (0.000000)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# definindo uma semente global\n",
    "np.random.seed(7) \n",
    "\n",
    "# Avaliação dos modelos\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=num_folds)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = '%s: %f (%f)' % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "\n",
    "# Comparação dos modelos\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Comparação dos Modelos')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3e3fb3",
   "metadata": {},
   "source": [
    "#### 5.2 - Criação e avaliação de modelos: dados padronizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da29cd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledKNN: 0.976389 (0.047324)\n",
      "ScaledCART: 0.976389 (0.047324)\n",
      "ScaledNB: 0.965278 (0.072768)\n",
      "ScaledSVM: 0.988889 (0.033333)\n",
      "ScaledLR: 0.988889 (0.033333)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parâmetros\n",
    "num_folds = 10\n",
    "scoring = 'accuracy'\n",
    "\n",
    "# definindo uma semente global\n",
    "np.random.seed(7) \n",
    "\n",
    "# Padronização do dataset\n",
    "pipelines = []\n",
    "\n",
    "pipelines.append(('ScaledKNN', \n",
    "                  Pipeline([('Scaler', StandardScaler()),\n",
    "                            ('KNN', KNeighborsClassifier(metric = 'euclidean',\n",
    "                                                         n_neighbors = 1))])))\n",
    "\n",
    "pipelines.append(('ScaledCART', \n",
    "                  Pipeline([('Scaler', StandardScaler()),\n",
    "                            ('CART', DecisionTreeClassifier(criterion = 'gini',\n",
    "                                                            max_depth = 10,\n",
    "                                                            min_samples_leaf = 1,\n",
    "                                                            min_samples_split = 5))])))\n",
    "\n",
    "pipelines.append(('ScaledNB', \n",
    "                  Pipeline([('Scaler', StandardScaler()),\n",
    "                            ('NB', GaussianNB(var_smoothing = 5e-05))])))\n",
    "\n",
    "pipelines.append(('ScaledSVM', \n",
    "                  Pipeline([('Scaler', StandardScaler()),\n",
    "                            ('SVM', SVC(C = 0.1,\n",
    "                                       kernel = 'linear'))])))\n",
    "\n",
    "pipelines.append(('ScaledLR', \n",
    "                  Pipeline([('Scaler', StandardScaler()), \n",
    "                                        ('LR', LogisticRegression(C= 0.01,\n",
    "                                                                  penalty= None,\n",
    "                                                                  solver= 'lbfgs'))])))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "    \n",
    "for name, model in pipelines:\n",
    "    kfold = KFold(n_splits=num_folds)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = '%s: %f (%f)' % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg) \n",
    "    \n",
    "# Comparação dos modelos\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Comparação dos Modelos')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e072ae9e",
   "metadata": {},
   "source": [
    "#### 6 - Apresentação dos resultados - DADOS FINAIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e9913d",
   "metadata": {},
   "source": [
    "##### 6.1 - Sem normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ed1bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ============================================\n",
    "#\n",
    "# IMPORTANDO BIBLIOTECAS\n",
    "#\n",
    "# ============================================\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# ============================================\n",
    "# ============================================\n",
    "#\n",
    "# IMPORTANDO ARQUIVOS\n",
    "#\n",
    "# ============================================\n",
    "# ============================================\n",
    "\n",
    "# nome do arquivo\n",
    "\n",
    "dir_treino = 'Modulo_Fase_TREINO.txt' \n",
    "dir_teste ='Modulo_Fase_TESTE.txt'\n",
    "\n",
    "\n",
    "# Informa o cabeçalho das colunas\n",
    "colunas = ['ID_TRILHA', 'G_SAÚDE', 'G_ISOLAMENTO', 'G_TRILHA', 'N_ISOLAMENTO', 'N_TRILHA', 'M_225886', 'M_246180', 'M_268298', 'M_292402', 'M_318672', 'M_347302', 'M_378504', 'M_412509', 'M_449569', 'M_489959', 'M_533978', 'M_581951', 'M_634235', 'M_691215', 'M_753315', 'M_820994', 'M_894753', 'M_975139', 'M_1062747', 'M_1158226', 'M_1262283', 'M_1375688', 'M_1499282', 'M_1633980', 'M_1780779', 'M_1940767', 'M_2115128', 'M_2305154', 'M_2512253', 'M_2737957', 'M_2983939', 'M_3252021', 'M_3544187', 'M_3862602', 'M_4209624', 'M_4587823', 'F_225886', 'F_246180', 'F_268298', 'F_292402', 'F_318672', 'F_347302', 'F_378504', 'F_412509', 'F_449569', 'F_489959', 'F_533978', 'F_581951', 'F_634235', 'F_691215', 'F_753315', 'F_820994', 'F_894753', 'F_975139', 'F_1062747', 'F_1158226', 'F_1262283', 'F_1375688', 'F_1499282', 'F_1633980', 'F_1780779', 'F_1940767', 'F_2115128', 'F_2305154', 'F_2512253', 'F_2737957', 'F_2983939', 'F_3252021', 'F_3544187', 'F_3862602', 'F_4209624', 'F_4587823']\n",
    "# Carrega uma base de dados - TREINO\n",
    "dataset_treino = pd.read_csv(dir_treino, names=colunas, skiprows=0, delimiter=';') \n",
    "\n",
    "#Carrega uma base de dados - TESTE\n",
    "dataset_teste = pd.read_csv(dir_teste, names=colunas, skiprows=0, delimiter=';')   \n",
    "\n",
    "# ============================================\n",
    "# ============================================\n",
    "#\n",
    "# SEPARANDO GRUPOS DE TREINO E TESTE\n",
    "#\n",
    "# ============================================\n",
    "# ============================================\n",
    "\n",
    "array_treino = dataset_treino.values\n",
    "array_teste = dataset_teste.values\n",
    "\n",
    "X_train = array_treino [:,4:79].astype(float)\n",
    "X_test = array_teste [:,4:79].astype(float)\n",
    "\n",
    "Y_train =  array_treino [:,1].astype(int)\n",
    "Y_test =  array_teste [:,1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dcd07fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ============================================\n",
    "#\n",
    "# CRIANDO PARAMETROS E MODELOS DE APRENDIZAGEM DE MÁQUINA\n",
    "#\n",
    "# ============================================\n",
    "# ============================================\n",
    "\n",
    "### Parâmetros\n",
    "\n",
    "# número de fols - validação cruzada\n",
    "num_folds = 10\n",
    "\n",
    "# metrica\n",
    "scoring = 'accuracy'\n",
    "\n",
    "# definindo uma semente global\n",
    "np.random.seed(7) \n",
    "\n",
    "# ============================================\n",
    "# ============================================\n",
    "\n",
    "# Criação dos modelos\n",
    "\n",
    "model_KNN = KNeighborsClassifier(metric='euclidean',n_neighbors=1)\n",
    "model_CART = DecisionTreeClassifier(criterion='gini',max_depth=10,min_samples_leaf=1,min_samples_split=5)\n",
    "model_NB = GaussianNB(var_smoothing=5e-05)\n",
    "model_SVN = SVC(C=0.1,kernel='linear')\n",
    "model_LR = LogisticRegression(C= 0.01,penalty= None,solver='lbfgs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e263bbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN: 0.427778 (0.179463)\n",
      "CART: 0.920833 (0.199348)\n",
      "NB: 0.616667 (0.190151)\n",
      "SVN: 0.987500 (0.037500)\n",
      "LR: 0.937500 (0.150520)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ============================================\n",
    "# RESULTADO DA MÉDIA DOS FOLDS - POR VALIDAÇÃO CRUZADA\n",
    "# ============================================\n",
    "# ============================================\n",
    "\n",
    "kfold = KFold(n_splits=num_folds)\n",
    "\n",
    "\n",
    "cv_results_KNN = cross_val_score(model_KNN, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "msg = '%s: %f (%f)' % ('KNN', cv_results_KNN.mean(), cv_results_KNN.std())\n",
    "print(msg)\n",
    "\n",
    "cv_results_CART = cross_val_score(model_CART, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "msg = '%s: %f (%f)' % ('CART', cv_results_CART.mean(), cv_results_CART.std())\n",
    "print(msg)\n",
    "\n",
    "cv_results_NB = cross_val_score(model_NB, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "msg = '%s: %f (%f)' % ('NB', cv_results_NB.mean(), cv_results_NB.std())\n",
    "print(msg)\n",
    "\n",
    "cv_results_SVN = cross_val_score(model_SVN, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "msg = '%s: %f (%f)' % ('SVN', cv_results_SVN.mean(), cv_results_SVN.std())\n",
    "print(msg)\n",
    "\n",
    "cv_results_LR = cross_val_score(model_LR, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "msg = '%s: %f (%f)' % ('LR', cv_results_LR.mean(), cv_results_LR.std())\n",
    "print(msg)\n",
    "\n",
    "# ============================================\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e9c3cbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======\n",
      "=======\n",
      "KNN - f1-score - macro =  0.40888888888888886\n",
      "KNN - Precision score =  0.15789473684210525\n",
      "KNN - Accuracy score =  0.45714285714285713\n",
      "=======\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Trilha com falha       0.81      0.45      0.58        29\n",
      "Trilha sem falha       0.16      0.50      0.24         6\n",
      "\n",
      "        accuracy                           0.46        35\n",
      "       macro avg       0.49      0.47      0.41        35\n",
      "    weighted avg       0.70      0.46      0.52        35\n",
      "\n",
      "=======\n",
      "=======\n",
      "CART - f1-score - macro =  0.9527665317139\n",
      "CART - Precision score =  0.8571428571428571\n",
      "CART - Accuracy score =  0.9714285714285714\n",
      "=======\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Trilha com falha       1.00      0.97      0.98        29\n",
      "Trilha sem falha       0.86      1.00      0.92         6\n",
      "\n",
      "        accuracy                           0.97        35\n",
      "       macro avg       0.93      0.98      0.95        35\n",
      "    weighted avg       0.98      0.97      0.97        35\n",
      "\n",
      "=======\n",
      "=======\n",
      "NB - f1-score - macro =  0.51\n",
      "NB - Precision score =  0.21428571428571427\n",
      "NB - Accuracy score =  0.6\n",
      "=======\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Trilha com falha       0.86      0.62      0.72        29\n",
      "Trilha sem falha       0.21      0.50      0.30         6\n",
      "\n",
      "        accuracy                           0.60        35\n",
      "       macro avg       0.54      0.56      0.51        35\n",
      "    weighted avg       0.75      0.60      0.65        35\n",
      "\n",
      "=======\n",
      "=======\n",
      "SVN - f1-score - macro =  1.0\n",
      "SVN - Precision score =  1.0\n",
      "SVN - Accuracy score =  1.0\n",
      "=======\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Trilha com falha       1.00      1.00      1.00        29\n",
      "Trilha sem falha       1.00      1.00      1.00         6\n",
      "\n",
      "        accuracy                           1.00        35\n",
      "       macro avg       1.00      1.00      1.00        35\n",
      "    weighted avg       1.00      1.00      1.00        35\n",
      "\n",
      "=======\n",
      "=======\n",
      "LR - f1-score - macro =  1.0\n",
      "LR - Precision score =  1.0\n",
      "LR - Accuracy score =  1.0\n",
      "=======\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Trilha com falha       1.00      1.00      1.00        29\n",
      "Trilha sem falha       1.00      1.00      1.00         6\n",
      "\n",
      "        accuracy                           1.00        35\n",
      "       macro avg       1.00      1.00      1.00        35\n",
      "    weighted avg       1.00      1.00      1.00        35\n",
      "\n",
      "=======\n",
      "=======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ============================================\n",
    "# METRICAS USANDO OS VALORES DE TESTE\n",
    "# ============================================\n",
    "# ============================================\n",
    "\n",
    "labels = ['Trilha com falha', 'Trilha sem falha']\n",
    "\n",
    "# ============================================\n",
    "# KNN\n",
    "# ============================================\n",
    "\n",
    "model_KNN.fit(X_train, Y_train)\n",
    "y_pred_KNN = model_KNN.predict(X_test)\n",
    "\n",
    "print('=======')\n",
    "print('=======')\n",
    "print('KNN - f1-score - macro = ', f1_score(Y_test, y_pred_KNN, average='macro')) #,labels=None))\n",
    "print('KNN - Precision score = ', precision_score(Y_test, y_pred_KNN))\n",
    "print('KNN - Accuracy score = ', accuracy_score(Y_test, y_pred_KNN))\n",
    "print('=======')\n",
    "print(classification_report(Y_test, y_pred_KNN, target_names=labels))\n",
    "print('=======')\n",
    "\n",
    "# ============================================\n",
    "# CART\n",
    "# ============================================\n",
    "\n",
    "model_CART.fit(X_train, Y_train)\n",
    "y_pred_CART = model_CART.predict(X_test)\n",
    "\n",
    "print('=======')\n",
    "print('CART - f1-score - macro = ', f1_score(Y_test, y_pred_CART, average='macro')) #,labels=None))\n",
    "print('CART - Precision score = ', precision_score(Y_test, y_pred_CART))\n",
    "print('CART - Accuracy score = ', accuracy_score(Y_test, y_pred_CART))\n",
    "print('=======')\n",
    "print(classification_report(Y_test, y_pred_CART, target_names=labels))\n",
    "print('=======')\n",
    "\n",
    "# ============================================\n",
    "# NB\n",
    "# ============================================\n",
    "\n",
    "model_NB.fit(X_train, Y_train)\n",
    "y_pred_NB = model_NB.predict(X_test)\n",
    "\n",
    "print('=======')\n",
    "print('NB - f1-score - macro = ', f1_score(Y_test, y_pred_NB, average='macro')) #,labels=None))\n",
    "print('NB - Precision score = ', precision_score(Y_test, y_pred_NB))\n",
    "print('NB - Accuracy score = ', accuracy_score(Y_test, y_pred_NB))\n",
    "print('=======')\n",
    "print(classification_report(Y_test, y_pred_NB, target_names=labels))\n",
    "print('=======')\n",
    "\n",
    "# ============================================\n",
    "# SVN\n",
    "# ============================================\n",
    "\n",
    "model_SVN.fit(X_train, Y_train)\n",
    "y_pred_SVN = model_SVN.predict(X_test)\n",
    "\n",
    "print('=======')\n",
    "print('SVN - f1-score - macro = ', f1_score(Y_test, y_pred_SVN, average='macro')) #,labels=None))\n",
    "print('SVN - Precision score = ', precision_score(Y_test, y_pred_SVN))\n",
    "print('SVN - Accuracy score = ', accuracy_score(Y_test, y_pred_SVN))\n",
    "print('=======')\n",
    "print(classification_report(Y_test, y_pred_SVN, target_names=labels))\n",
    "print('=======')\n",
    "\n",
    "# ============================================\n",
    "# LR\n",
    "# ============================================\n",
    "\n",
    "model_LR.fit(X_train, Y_train)\n",
    "y_pred_LR = model_LR.predict(X_test)\n",
    "\n",
    "print('=======')\n",
    "print('LR - f1-score - macro = ', f1_score(Y_test, y_pred_LR, average='macro')) #,labels=None))\n",
    "print('LR - Precision score = ', precision_score(Y_test, y_pred_LR))\n",
    "print('LR - Accuracy score = ', accuracy_score(Y_test, y_pred_LR))\n",
    "print('=======')\n",
    "print(classification_report(Y_test, y_pred_LR, target_names=labels))\n",
    "print('=======')\n",
    "print('=======')\n",
    "    \n",
    "\n",
    "# ============================================\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4d1946f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================\n",
    "# ============================================\n",
    "# METRICAS USANDO OS VALORES DE TESTE\n",
    "# ============================================\n",
    "# ============================================\n",
    "\n",
    "# Criação dos subplots\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(10, 5))\n",
    "\n",
    "##########################\n",
    "# 1 - model_KNN\n",
    "##########################\n",
    "\n",
    "# Matriz de confusão - SVN\n",
    "CM_KNN = confusion_matrix(Y_test, y_pred_KNN)\n",
    "disp0 = ConfusionMatrixDisplay(confusion_matrix=CM_KNN)\n",
    "disp0.plot(ax=axes[0], cmap='Reds', values_format='d', colorbar=False)\n",
    "#\n",
    "axes[0].set_title('Classificador 1\\n')#\\n\n",
    "axes[0].set_xlabel('Prevista')\n",
    "axes[0].set_ylabel('Real')\n",
    "axes[0].set_xticklabels(['0','1'], rotation='horizontal',ha='center')\n",
    "axes[0].set_yticklabels(['0  ','1  '], rotation='horizontal',ha='center')\n",
    "\n",
    "##########################\n",
    "# 2 - model_CART\n",
    "##########################\n",
    "\n",
    "# Matriz de confusão - CART\n",
    "CM_CART = confusion_matrix(Y_test, y_pred_CART)\n",
    "disp1 = ConfusionMatrixDisplay(confusion_matrix=CM_CART)\n",
    "disp1.plot(ax=axes[1], cmap='Greens', values_format='d', colorbar=False)\n",
    "#\n",
    "axes[1].set_title('Classificador 2\\n')#\\n\n",
    "axes[1].set_xlabel('Prevista')\n",
    "axes[1].set_ylabel('Real')\n",
    "axes[1].set_xticklabels(['0','1'], rotation='horizontal',ha='center')\n",
    "axes[1].set_yticklabels(['0  ','1  '], rotation='horizontal',ha='center')\n",
    "\n",
    "##########################\n",
    "# 3 - model_NB\n",
    "##########################\n",
    "\n",
    "# Matriz de confusão - NB\n",
    "CM_NB = confusion_matrix(Y_test, y_pred_NB)\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=CM_NB)\n",
    "disp2.plot(ax=axes[2], cmap='Blues', values_format='d', colorbar=False)\n",
    "#\n",
    "axes[2].set_title('Classificador 3\\n')#\\n\n",
    "axes[2].set_xlabel('Prevista')\n",
    "axes[2].set_ylabel('Real')\n",
    "axes[2].set_xticklabels(['0','1'], rotation='horizontal',ha='center')\n",
    "axes[2].set_yticklabels(['0  ','1  '], rotation='horizontal',ha='center')\n",
    "\n",
    "##########################\n",
    "# 4 - model_SVN\n",
    "##########################\n",
    "\n",
    "# Matriz de confusão - SVN\n",
    "CM_SVN = confusion_matrix(Y_test, y_pred_SVN)\n",
    "disp3 = ConfusionMatrixDisplay(confusion_matrix=CM_SVN)\n",
    "disp3.plot(ax=axes[3], cmap='Oranges', values_format='d', colorbar=False)\n",
    "#\n",
    "axes[3].set_title('Classificador 4\\n')#\\n\n",
    "axes[3].set_xlabel('Prevista')\n",
    "axes[3].set_ylabel('Real')\n",
    "axes[3].set_xticklabels(['0','1'], rotation='horizontal',ha='center')\n",
    "axes[3].set_yticklabels(['0  ','1  '], rotation='horizontal',ha='center')\n",
    "\n",
    "##########################\n",
    "# 5 - model_LR\n",
    "##########################\n",
    "\n",
    "# Matriz de confusão - LR\n",
    "CM_LR = confusion_matrix(Y_test, y_pred_LR)\n",
    "disp4 = ConfusionMatrixDisplay(confusion_matrix=CM_LR)\n",
    "disp4.plot(ax=axes[4], cmap='Greys', values_format='d', colorbar=False)\n",
    "#\n",
    "axes[4].set_title('Classificador 5\\n')#\\n\n",
    "axes[4].set_xlabel('Prevista')\n",
    "axes[4].set_ylabel('Real')\n",
    "axes[4].set_xticklabels(['0','1'], rotation='horizontal',ha='center')\n",
    "axes[4].set_yticklabels(['0  ','1  '], rotation='horizontal',ha='center')\n",
    "##########################\n",
    "##########################\n",
    "\n",
    "# Ajusta o layout e exibe o gráfico\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b79939",
   "metadata": {},
   "source": [
    "##### 6.2 - Com normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f3555cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ============================================\n",
    "#\n",
    "# IMPORTANDO BIBLIOTECAS\n",
    "#\n",
    "# ============================================\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# ============================================\n",
    "# ============================================\n",
    "#\n",
    "# IMPORTANDO ARQUIVOS\n",
    "#\n",
    "# ============================================\n",
    "# ============================================\n",
    "\n",
    "# nome do arquivo\n",
    "\n",
    "dir_treino = 'Modulo_Fase_TREINO.txt' \n",
    "dir_teste ='Modulo_Fase_TESTE.txt'\n",
    "\n",
    "\n",
    "# Informa o cabeçalho das colunas\n",
    "colunas = ['ID_TRILHA', 'G_SAÚDE', 'G_ISOLAMENTO', 'G_TRILHA', 'N_ISOLAMENTO', 'N_TRILHA', 'M_225886', 'M_246180', 'M_268298', 'M_292402', 'M_318672', 'M_347302', 'M_378504', 'M_412509', 'M_449569', 'M_489959', 'M_533978', 'M_581951', 'M_634235', 'M_691215', 'M_753315', 'M_820994', 'M_894753', 'M_975139', 'M_1062747', 'M_1158226', 'M_1262283', 'M_1375688', 'M_1499282', 'M_1633980', 'M_1780779', 'M_1940767', 'M_2115128', 'M_2305154', 'M_2512253', 'M_2737957', 'M_2983939', 'M_3252021', 'M_3544187', 'M_3862602', 'M_4209624', 'M_4587823', 'F_225886', 'F_246180', 'F_268298', 'F_292402', 'F_318672', 'F_347302', 'F_378504', 'F_412509', 'F_449569', 'F_489959', 'F_533978', 'F_581951', 'F_634235', 'F_691215', 'F_753315', 'F_820994', 'F_894753', 'F_975139', 'F_1062747', 'F_1158226', 'F_1262283', 'F_1375688', 'F_1499282', 'F_1633980', 'F_1780779', 'F_1940767', 'F_2115128', 'F_2305154', 'F_2512253', 'F_2737957', 'F_2983939', 'F_3252021', 'F_3544187', 'F_3862602', 'F_4209624', 'F_4587823']\n",
    "# Carrega uma base de dados - TREINO\n",
    "dataset_treino = pd.read_csv(dir_treino, names=colunas, skiprows=0, delimiter=';') \n",
    "\n",
    "#Carrega uma base de dados - TESTE\n",
    "dataset_teste = pd.read_csv(dir_teste, names=colunas, skiprows=0, delimiter=';')   \n",
    "\n",
    "# ============================================\n",
    "# ============================================\n",
    "#\n",
    "# SEPARANDO GRUPOS DE TREINO E TESTE\n",
    "#\n",
    "# ============================================\n",
    "# ============================================\n",
    "\n",
    "array_treino = dataset_treino.values\n",
    "array_teste = dataset_teste.values\n",
    "\n",
    "X_train = array_treino [:,4:79].astype(float)\n",
    "X_test = array_teste [:,4:79].astype(float)\n",
    "\n",
    "Y_train =  array_treino [:,1].astype(int)\n",
    "Y_test =  array_teste [:,1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "38ed4ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------\n",
    "# Normaliza dados \n",
    "#------------------------\n",
    "\n",
    "\n",
    "# Normaliza dados - TRAINO\n",
    "\n",
    "scaler_train = StandardScaler().fit(X_train)\n",
    "X_train_rescaled = scaler_train.transform(X_train)\n",
    "\n",
    "# Normaliza dados - TESTE\n",
    "\n",
    "scaler_test = StandardScaler().fit(X_test)\n",
    "X_test_rescaled = scaler_test.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0d94586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ============================================\n",
    "#\n",
    "# CRIANDO PARAMETROS E MODELOS DE APRENDIZAGEM DE MÁQUINA\n",
    "#\n",
    "# ============================================\n",
    "# ============================================\n",
    "\n",
    "### Parâmetros\n",
    "\n",
    "# número de fols - validação cruzada\n",
    "num_folds = 10\n",
    "\n",
    "# metrica\n",
    "scoring = 'accuracy'\n",
    "\n",
    "# definindo uma semente global\n",
    "np.random.seed(7) \n",
    "\n",
    "# ============================================\n",
    "# ============================================\n",
    "\n",
    "# Criação dos modelos\n",
    "\n",
    "model_KNN = KNeighborsClassifier(metric='euclidean',n_neighbors=1)\n",
    "model_CART = DecisionTreeClassifier(criterion='gini',max_depth=10,min_samples_leaf=1,min_samples_split=5)\n",
    "model_NB = GaussianNB(var_smoothing=5e-05)\n",
    "model_SVN = SVC(C=0.1,kernel='linear')\n",
    "model_LR = LogisticRegression(C= 0.01,penalty= None,solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "81793c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN: 0.919444 (0.175000)\n",
      "CART: 0.920833 (0.199348)\n",
      "NB: 0.893056 (0.217666)\n",
      "SVN: 0.944444 (0.166667)\n",
      "LR: 0.955556 (0.133333)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ============================================\n",
    "# RESULTADO DA MÉDIA DOS FOLDS - POR VALIDAÇÃO CRUZADA\n",
    "# ============================================\n",
    "# ============================================\n",
    "\n",
    "kfold = KFold(n_splits=num_folds)\n",
    "\n",
    "\n",
    "cv_results_KNN = cross_val_score(model_KNN, X_train_rescaled, Y_train, cv=kfold, scoring=scoring)\n",
    "msg = '%s: %f (%f)' % ('KNN', cv_results_KNN.mean(), cv_results_KNN.std())\n",
    "print(msg)\n",
    "\n",
    "cv_results_CART = cross_val_score(model_CART, X_train_rescaled, Y_train, cv=kfold, scoring=scoring)\n",
    "msg = '%s: %f (%f)' % ('CART', cv_results_CART.mean(), cv_results_CART.std())\n",
    "print(msg)\n",
    "\n",
    "cv_results_NB = cross_val_score(model_NB, X_train_rescaled, Y_train, cv=kfold, scoring=scoring)\n",
    "msg = '%s: %f (%f)' % ('NB', cv_results_NB.mean(), cv_results_NB.std())\n",
    "print(msg)\n",
    "\n",
    "cv_results_SVN = cross_val_score(model_SVN, X_train_rescaled, Y_train, cv=kfold, scoring=scoring)\n",
    "msg = '%s: %f (%f)' % ('SVN', cv_results_SVN.mean(), cv_results_SVN.std())\n",
    "print(msg)\n",
    "\n",
    "cv_results_LR = cross_val_score(model_LR, X_train_rescaled, Y_train, cv=kfold, scoring=scoring)\n",
    "msg = '%s: %f (%f)' % ('LR', cv_results_LR.mean(), cv_results_LR.std())\n",
    "print(msg)\n",
    "\n",
    "# ============================================\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "55cb9e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======\n",
      "=======\n",
      "KNN - f1-score - macro =  0.8379629629629629\n",
      "KNN - Precision score =  0.6\n",
      "KNN - Accuracy score =  0.8857142857142857\n",
      "=======\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Trilha com falha       1.00      0.86      0.93        29\n",
      "Trilha sem falha       0.60      1.00      0.75         6\n",
      "\n",
      "        accuracy                           0.89        35\n",
      "       macro avg       0.80      0.93      0.84        35\n",
      "    weighted avg       0.93      0.89      0.90        35\n",
      "\n",
      "=======\n",
      "=======\n",
      "CART - f1-score - macro =  0.9107142857142857\n",
      "CART - Precision score =  0.75\n",
      "CART - Accuracy score =  0.9428571428571428\n",
      "=======\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Trilha com falha       1.00      0.93      0.96        29\n",
      "Trilha sem falha       0.75      1.00      0.86         6\n",
      "\n",
      "        accuracy                           0.94        35\n",
      "       macro avg       0.88      0.97      0.91        35\n",
      "    weighted avg       0.96      0.94      0.95        35\n",
      "\n",
      "=======\n",
      "=======\n",
      "NB - f1-score - macro =  0.8382126348228043\n",
      "NB - Precision score =  0.8\n",
      "NB - Accuracy score =  0.9142857142857143\n",
      "=======\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Trilha com falha       0.93      0.97      0.95        29\n",
      "Trilha sem falha       0.80      0.67      0.73         6\n",
      "\n",
      "        accuracy                           0.91        35\n",
      "       macro avg       0.87      0.82      0.84        35\n",
      "    weighted avg       0.91      0.91      0.91        35\n",
      "\n",
      "=======\n",
      "=======\n",
      "SVN - f1-score - macro =  0.9107142857142857\n",
      "SVN - Precision score =  0.75\n",
      "SVN - Accuracy score =  0.9428571428571428\n",
      "=======\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Trilha com falha       1.00      0.93      0.96        29\n",
      "Trilha sem falha       0.75      1.00      0.86         6\n",
      "\n",
      "        accuracy                           0.94        35\n",
      "       macro avg       0.88      0.97      0.91        35\n",
      "    weighted avg       0.96      0.94      0.95        35\n",
      "\n",
      "=======\n",
      "=======\n",
      "LR - f1-score - macro =  0.9527665317139\n",
      "LR - Precision score =  0.8571428571428571\n",
      "LR - Accuracy score =  0.9714285714285714\n",
      "=======\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Trilha com falha       1.00      0.97      0.98        29\n",
      "Trilha sem falha       0.86      1.00      0.92         6\n",
      "\n",
      "        accuracy                           0.97        35\n",
      "       macro avg       0.93      0.98      0.95        35\n",
      "    weighted avg       0.98      0.97      0.97        35\n",
      "\n",
      "=======\n",
      "=======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ============================================\n",
    "# METRICAS USANDO OS VALORES DE TESTE\n",
    "# ============================================\n",
    "# ============================================\n",
    "\n",
    "labels = ['Trilha com falha', 'Trilha sem falha']\n",
    "\n",
    "# ============================================\n",
    "# KNN\n",
    "# ============================================\n",
    "\n",
    "model_KNN.fit(X_train_rescaled, Y_train)\n",
    "y_pred_KNN = model_KNN.predict(X_test_rescaled)\n",
    "\n",
    "print('=======')\n",
    "print('=======')\n",
    "print('KNN - f1-score - macro = ', f1_score(Y_test, y_pred_KNN, average='macro')) #,labels=None))\n",
    "print('KNN - Precision score = ', precision_score(Y_test, y_pred_KNN))\n",
    "print('KNN - Accuracy score = ', accuracy_score(Y_test, y_pred_KNN))\n",
    "print('=======')\n",
    "print(classification_report(Y_test, y_pred_KNN, target_names=labels))\n",
    "print('=======')\n",
    "\n",
    "# ============================================\n",
    "# CART\n",
    "# ============================================\n",
    "\n",
    "model_CART.fit(X_train_rescaled, Y_train)\n",
    "y_pred_CART = model_CART.predict(X_test_rescaled)\n",
    "\n",
    "print('=======')\n",
    "print('CART - f1-score - macro = ', f1_score(Y_test, y_pred_CART, average='macro')) #,labels=None))\n",
    "print('CART - Precision score = ', precision_score(Y_test, y_pred_CART))\n",
    "print('CART - Accuracy score = ', accuracy_score(Y_test, y_pred_CART))\n",
    "print('=======')\n",
    "print(classification_report(Y_test, y_pred_CART, target_names=labels))\n",
    "print('=======')\n",
    "\n",
    "# ============================================\n",
    "# NB\n",
    "# ============================================\n",
    "\n",
    "model_NB.fit(X_train_rescaled, Y_train)\n",
    "y_pred_NB = model_NB.predict(X_test_rescaled)\n",
    "\n",
    "print('=======')\n",
    "print('NB - f1-score - macro = ', f1_score(Y_test, y_pred_NB, average='macro')) #,labels=None))\n",
    "print('NB - Precision score = ', precision_score(Y_test, y_pred_NB))\n",
    "print('NB - Accuracy score = ', accuracy_score(Y_test, y_pred_NB))\n",
    "print('=======')\n",
    "print(classification_report(Y_test, y_pred_NB, target_names=labels))\n",
    "print('=======')\n",
    "\n",
    "# ============================================\n",
    "# SVN\n",
    "# ============================================\n",
    "\n",
    "model_SVN.fit(X_train_rescaled, Y_train)\n",
    "y_pred_SVN = model_SVN.predict(X_test_rescaled)\n",
    "\n",
    "print('=======')\n",
    "print('SVN - f1-score - macro = ', f1_score(Y_test, y_pred_SVN, average='macro')) #,labels=None))\n",
    "print('SVN - Precision score = ', precision_score(Y_test, y_pred_SVN))\n",
    "print('SVN - Accuracy score = ', accuracy_score(Y_test, y_pred_SVN))\n",
    "print('=======')\n",
    "print(classification_report(Y_test, y_pred_SVN, target_names=labels))\n",
    "print('=======')\n",
    "\n",
    "# ============================================\n",
    "# LR\n",
    "# ============================================\n",
    "\n",
    "model_LR.fit(X_train_rescaled, Y_train)\n",
    "y_pred_LR = model_LR.predict(X_test_rescaled)\n",
    "\n",
    "print('=======')\n",
    "print('LR - f1-score - macro = ', f1_score(Y_test, y_pred_LR, average='macro')) #,labels=None))\n",
    "print('LR - Precision score = ', precision_score(Y_test, y_pred_LR))\n",
    "print('LR - Accuracy score = ', accuracy_score(Y_test, y_pred_LR))\n",
    "print('=======')\n",
    "print(classification_report(Y_test, y_pred_LR, target_names=labels))\n",
    "print('=======')\n",
    "print('=======')\n",
    "    \n",
    "\n",
    "# ============================================\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bbdfaf7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================\n",
    "# ============================================\n",
    "# METRICAS USANDO OS VALORES DE TESTE\n",
    "# ============================================\n",
    "# ============================================\n",
    "\n",
    "# Criação dos subplots\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(10, 5))\n",
    "\n",
    "##########################\n",
    "# 1 - model_KNN\n",
    "##########################\n",
    "\n",
    "# Matriz de confusão - SVN\n",
    "CM_KNN = confusion_matrix(Y_test, y_pred_KNN)\n",
    "disp0 = ConfusionMatrixDisplay(confusion_matrix=CM_KNN)\n",
    "disp0.plot(ax=axes[0], cmap='Reds', values_format='d', colorbar=False)\n",
    "#\n",
    "axes[0].set_title('Classificador 1\\n')#\\n\n",
    "axes[0].set_xlabel('Prevista')\n",
    "axes[0].set_ylabel('Real')\n",
    "axes[0].set_xticklabels(['0','1'], rotation='horizontal',ha='center')\n",
    "axes[0].set_yticklabels(['0  ','1  '], rotation='horizontal',ha='center')\n",
    "\n",
    "##########################\n",
    "# 2 - model_CART\n",
    "##########################\n",
    "\n",
    "# Matriz de confusão - CART\n",
    "CM_CART = confusion_matrix(Y_test, y_pred_CART)\n",
    "disp1 = ConfusionMatrixDisplay(confusion_matrix=CM_CART)\n",
    "disp1.plot(ax=axes[1], cmap='Greens', values_format='d', colorbar=False)\n",
    "#\n",
    "axes[1].set_title('Classificador 2\\n')#\\n\n",
    "axes[1].set_xlabel('Prevista')\n",
    "axes[1].set_ylabel('Real')\n",
    "axes[1].set_xticklabels(['0','1'], rotation='horizontal',ha='center')\n",
    "axes[1].set_yticklabels(['0  ','1  '], rotation='horizontal',ha='center')\n",
    "\n",
    "##########################\n",
    "# 3 - model_NB\n",
    "##########################\n",
    "\n",
    "# Matriz de confusão - NB\n",
    "CM_NB = confusion_matrix(Y_test, y_pred_NB)\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=CM_NB)\n",
    "disp2.plot(ax=axes[2], cmap='Blues', values_format='d', colorbar=False)\n",
    "#\n",
    "axes[2].set_title('Classificador 3\\n')#\\n\n",
    "axes[2].set_xlabel('Prevista')\n",
    "axes[2].set_ylabel('Real')\n",
    "axes[2].set_xticklabels(['0','1'], rotation='horizontal',ha='center')\n",
    "axes[2].set_yticklabels(['0  ','1  '], rotation='horizontal',ha='center')\n",
    "\n",
    "##########################\n",
    "# 4 - model_SVN\n",
    "##########################\n",
    "\n",
    "# Matriz de confusão - SVN\n",
    "CM_SVN = confusion_matrix(Y_test, y_pred_SVN)\n",
    "disp3 = ConfusionMatrixDisplay(confusion_matrix=CM_SVN)\n",
    "disp3.plot(ax=axes[3], cmap='Oranges', values_format='d', colorbar=False)\n",
    "#\n",
    "axes[3].set_title('Classificador 4\\n')#\\n\n",
    "axes[3].set_xlabel('Prevista')\n",
    "axes[3].set_ylabel('Real')\n",
    "axes[3].set_xticklabels(['0','1'], rotation='horizontal',ha='center')\n",
    "axes[3].set_yticklabels(['0  ','1  '], rotation='horizontal',ha='center')\n",
    "\n",
    "##########################\n",
    "# 5 - model_LR\n",
    "##########################\n",
    "\n",
    "# Matriz de confusão - LR\n",
    "CM_LR = confusion_matrix(Y_test, y_pred_LR)\n",
    "disp4 = ConfusionMatrixDisplay(confusion_matrix=CM_LR)\n",
    "disp4.plot(ax=axes[4], cmap='Greys', values_format='d', colorbar=False)\n",
    "#\n",
    "axes[4].set_title('Classificador 5\\n')#\\n\n",
    "axes[4].set_xlabel('Prevista')\n",
    "axes[4].set_ylabel('Real')\n",
    "axes[4].set_xticklabels(['0','1'], rotation='horizontal',ha='center')\n",
    "axes[4].set_yticklabels(['0  ','1  '], rotation='horizontal',ha='center')\n",
    "##########################\n",
    "##########################\n",
    "\n",
    "# Ajusta o layout e exibe o gráfico\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# ============================================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
